{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "sys.path.append(\"/home/lauro/code/msc-thesis/svgd/kernel_learning\")\n",
    "import json\n",
    "import collections\n",
    "import itertools\n",
    "from functools import partial\n",
    "import importlib\n",
    "\n",
    "import numpy as onp\n",
    "from jax.config import config\n",
    "# config.update(\"jax_log_compiles\", True)\n",
    "# config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, random, lax, jacfwd, value_and_grad\n",
    "from jax import lax\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as onp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import haiku as hk\n",
    "import ot\n",
    "\n",
    "import config\n",
    "\n",
    "import utils\n",
    "import metrics\n",
    "import time\n",
    "import plot\n",
    "import stein\n",
    "import kernels\n",
    "import distributions\n",
    "import nets\n",
    "import kernel_learning\n",
    "\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "from jax.scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientLearner():\n",
    "    \"\"\"Learn a gradient vector field to transport particles.\"\"\"\n",
    "    def __init__(self,\n",
    "                 key,\n",
    "                 target,\n",
    "                 sizes: list,\n",
    "                 learning_rate: float = 0.01)\n",
    "        self.target = target\n",
    "        self.threadkey, subkey = random.split(key)\n",
    "\n",
    "        self.opt = Optimizer(*optimizers.adam(learning_rate))\n",
    "        self.step_counter = 0\n",
    "        self.initialize_optimizer(subkey)\n",
    "        self.rundata = {}\n",
    "        self.frozen_states = []\n",
    "\n",
    "    def initialize_optimizer(self, key=None, keep_params=False):\n",
    "        \"\"\"Initialize optimizer. If keep_params=True, then only the learning\n",
    "        rate schedule is reinitialized, otherwise the model parameters are\n",
    "        also reinitialized.\"\"\"\n",
    "        if keep_params:\n",
    "            self.optimizer_state = self.opt.init(self.get_params())\n",
    "        else:\n",
    "            x_dummy = np.ones(self.target.d)\n",
    "            if key is None:\n",
    "                self.threadkey, subkey = random.split(self.threadkey)\n",
    "            else:\n",
    "                key, subkey = random.split(key)\n",
    "            init_params = self.mlp.init(subkey, x_dummy)\n",
    "            self.optimizer_state = self.opt.init(init_params)\n",
    "        return None\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.opt.get_params(self.optimizer_state)\n",
    "\n",
    "    def loss_fn(self, params, samples):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_gradient(params, key, particles):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _step_unjitted(self, optimizer_state, samples, step: int):\n",
    "        # update step\n",
    "        params = self.opt.get_params(optimizer_state)\n",
    "        [loss, loss_aux], g = value_and_grad(self.loss_fn, has_aux=True)(params, samples)\n",
    "        optimizer_state = self.opt.update(step, g, optimizer_state)\n",
    "        return optimizer_state, loss_aux\n",
    "    _step = jit(_step_unjitted, static_argnums=0)\n",
    "\n",
    "    def step(self, samples, disable_jit=False):\n",
    "        \"\"\"Step and mutate state\"\"\"\n",
    "        step_fn = self._step_unjitted if disable_jit else self._step\n",
    "        updated_optimizer_state, aux = step_fn(\n",
    "            self.optimizer_state, samples, self.step_counter)\n",
    "        if any([np.any(np.isnan(leaf)) 2 2\n",
    "                for leaf in tree_util.tree_leaves(updated_optimizer_state)]):\n",
    "            raise FloatingPointError(\"NaN detected!\")\n",
    "        self.optimizer_state = updated_optimizer_state\n",
    "        self.log(aux)\n",
    "        self.step_counter += 1\n",
    "        return None\n",
    "\n",
    "    def log(self, aux): # depends on loss_fn aux\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def validate(self, validation_samples):\n",
    "        params = self.get_params()\n",
    "        val_loss, _ = self.loss_fn(params, validation_samples)\n",
    "        metrics.append_to_log(self.rundata, {\n",
    "            \"validation_loss\": (self.step_counter, val_loss),\n",
    "            \"validation_sd\": (self.step_counter, val_sd),\n",
    "        })\n",
    "        return\n",
    "\n",
    "    def train(self, samples, validation_samples, key=None, n_steps=100, noise_level=0, catch_nan_errors=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        * samples: batch to train on\n",
    "        * proposal: distribution batch is sampled from. Used to validate (or to\n",
    "        sample when sample_every is True)\n",
    "        * batch_size: nr of samples (use when not passing samples)\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            self.threadkey, key = random.split(self.threadkey)\n",
    "\n",
    "        def step(key, samples):\n",
    "            step_samples = samples + random.normal(key, samples.shape)*noise_level\n",
    "            self.step(step_samples)\n",
    "            if self.step_counter % 20 == 0:\n",
    "                self.validate(validation_samples)\n",
    "\n",
    "        for _ in range(n_steps):\n",
    "            try:\n",
    "                key, subkey = random.split(key)\n",
    "                step(subkey, samples)\n",
    "            except FloatingPointError as err:\n",
    "                if catch_nan_errors:\n",
    "                    return\n",
    "                else:\n",
    "                    raise err from None\n",
    "        return\n",
    "\n",
    "    def train_sampling_every_time(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def freeze_state(self):\n",
    "        \"\"\"Stores current params, log, and step_counter\"\"\"\n",
    "        self.frozen_states.append((self.step_counter,\n",
    "                                   self.get_params(),\n",
    "                                   self.rundata))\n",
    "        return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
