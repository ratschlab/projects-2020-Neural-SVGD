{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../../../learning_particle_gradients/\")\n",
    "from jax import config\n",
    "config.update(\"jax_debug_nans\", False)\n",
    "from tqdm import tqdm\n",
    "from jax import config\n",
    "\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, random, lax, jacfwd, value_and_grad\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as onp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import haiku as hk\n",
    "    \n",
    "import utils\n",
    "import plot\n",
    "import distributions\n",
    "import stein\n",
    "import models\n",
    "import flows\n",
    "from itertools import cycle, islice\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from functools import partial\n",
    "import kernels\n",
    "import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "sns.set(style='white')\n",
    "\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpk = tfp.math.psd_kernels\n",
    "\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up exporting\n",
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "#     'font.family': 'serif',\n",
    "#     'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "    'axes.unicode_minus': False, # avoid unicode error on saving plots with negative numbers (??)\n",
    "})\n",
    "\n",
    "figure_path = \"/home/lauro/documents/msc-thesis/thesis/figures/\"\n",
    "# save figures by using plt.savefig('path/to/fig')\n",
    "# remember that latex textwidth is 5.4in\n",
    "# so use figsize=[5.4, 4], for example\n",
    "printsize = [5.4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('/home/lauro/code/msc-thesis/wang_svgd/data/covertype.mat')\n",
    "features = data['covtype'][:, 1:]\n",
    "features = onp.hstack([features, onp.ones([features.shape[0], 1])]) # add intercept term\n",
    "\n",
    "labels = data['covtype'][:, 0]\n",
    "labels[labels == 2] = 0\n",
    "\n",
    "xx, x_test, yy, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(xx, yy, test_size=0.1, random_state=0)\n",
    "\n",
    "num_features = features.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 128\n",
    "# num_datapoints = len(x_train)\n",
    "# num_batches = num_datapoints // batch_size\n",
    "\n",
    "\n",
    "# def get_batches(x, y, n_steps=num_batches*2, batch_size=batch_size):\n",
    "#     \"\"\"Split x and y into batches\"\"\"\n",
    "#     assert len(x) == len(y)\n",
    "#     assert x.ndim > y.ndim\n",
    "#     n = len(x)\n",
    "#     idxs = onp.random.choice(n, size=(n_steps, batch_size))\n",
    "#     for idx in idxs:\n",
    "#         yield x[idx], y[idx]\n",
    "# #     batch_cycle = cycle(zip(*[onp.array_split(data, len(data)//batch_size) for data in (x, y)]))\n",
    "# #     return islice(batch_cycle, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0, b0 = 1, 0.01 # hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy import stats, special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative model\n",
    "def sample_from_prior(key, num=100):\n",
    "    keya, keyb = random.split(key)\n",
    "    alpha = random.gamma(keya, a0, shape=(num,)) / b0\n",
    "    w = random.normal(keyb, shape=(num, num_features))\n",
    "    return w, np.log(alpha)\n",
    "\n",
    "\n",
    "def prior_logp(w, log_alpha):\n",
    "    \"\"\"\n",
    "    Returns logp(w, log_alpha) = sum_i(logp(wi, alphai))\n",
    "\n",
    "    w has shape (num_features,), or (n, num_features)\n",
    "    similarly, log_alpha may have shape () or (n,)\"\"\"\n",
    "    if log_alpha.ndim == 0:\n",
    "        assert w.ndim == 1\n",
    "    elif log_alpha.ndim == 1:\n",
    "        assert log_alpha.shape[0] == w.shape[0]\n",
    "\n",
    "    alpha = np.exp(log_alpha)\n",
    "    logp_alpha = np.sum(stats.gamma.logpdf(alpha, a0, scale=1/b0))\n",
    "    if w.ndim == 2:\n",
    "        logp_w = np.sum(vmap(lambda wi, alphai: stats.norm.logpdf(wi, scale=1/np.sqrt(alphai)))(w, alpha))\n",
    "    elif w.ndim == 1:\n",
    "        logp_w = np.sum(stats.norm.logpdf(w, scale=1/np.sqrt(alpha)))\n",
    "    else:\n",
    "        raise\n",
    "    return logp_alpha + logp_w\n",
    "\n",
    "\n",
    "def preds(x, w):\n",
    "    \"\"\"returns predicted p(y = 1| x, w)\n",
    "\n",
    "    x can have shape (n, num_features) or (num_features,).\n",
    "    w is a single param of shape (num_features,)\"\"\"\n",
    "    return special.expit(x @ w)\n",
    "\n",
    "\n",
    "def loglikelihood(y, x, w):\n",
    "    \"\"\"\n",
    "    compute log p(y | x, w) for a single parameter w of\n",
    "    shape (num_features,) and a batch of data (y, x) of\n",
    "    shape (m,) and (m, num_features)\n",
    "\n",
    "    log p(y | x, w) = sum_i(logp(yi| xi, w))\n",
    "    \"\"\"\n",
    "    y = ((y - 1/2)*2).astype(np.int32)\n",
    "    logits = x @ w\n",
    "    prob_y = special.expit(logits*y)\n",
    "    return np.sum(np.log(prob_y))\n",
    "\n",
    "\n",
    "def log_posterior_unnormalized(y, x, w, log_alpha):\n",
    "    \"\"\"All is batched\"\"\"\n",
    "    log_prior = prior_logp(w, log_alpha)\n",
    "    log_likelihood = np.sum(vmap(lambda wi: loglikelihood(y, x, wi))(w))\n",
    "    return log_prior + log_likelihood\n",
    "\n",
    "\n",
    "def log_posterior_unnormalized_single_param(y, x, w, log_alpha):\n",
    "    \"\"\"y, x are batched, w, log_alpha not. In case I need\n",
    "    an unbatched eval of the target logp.\"\"\"\n",
    "    log_prior = prior_logp(w, log_alpha)\n",
    "    log_likelihood = loglikelihood(y, x, w)\n",
    "    return log_prior + log_likelihood\n",
    "\n",
    "\n",
    "def compute_probs(y, x, w):\n",
    "    \"\"\"\n",
    "    returns P(y_generated==y | x, w)\n",
    "\n",
    "    y and x are data batches. w is a single parameter\n",
    "    array of shape (num_features,)\"\"\"\n",
    "    y = ((y - 1/2)*2).astype(np.int32)\n",
    "    logits = x @ w\n",
    "    prob_y = special.expit(logits*y)\n",
    "    return prob_y\n",
    "\n",
    "\n",
    "@jit\n",
    "def compute_test_accuracy(w):\n",
    "    \"w is a batch\"\n",
    "    probs = vmap(lambda wi: compute_probs(y_test, x_test, wi))(w)\n",
    "    probs_y = np.mean(probs, axis=0)\n",
    "    return np.mean(probs_y > 0.5)\n",
    "\n",
    "\n",
    "@jit\n",
    "def compute_train_accuracy(w):\n",
    "    probs = vmap(lambda wi: compute_probs(y_train, x_train, wi))(w)\n",
    "    probs_y = np.mean(probs, axis=0)\n",
    "    return np.mean(probs_y > 0.5)\n",
    "\n",
    "\n",
    "def ravel(w, log_alpha):\n",
    "    return np.hstack([w, np.expand_dims(log_alpha, -1)])\n",
    "\n",
    "\n",
    "def unravel(params):\n",
    "    if params.ndim == 1:\n",
    "        return params[:-1], params[-1]\n",
    "    elif params.ndim == 2:\n",
    "        return params[:, :-1], np.squeeze(params[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_log_prob(raveled_params):\n",
    "    return log_posterior_unnormalized_single_param(yy, xx, *unravel(raveled_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chains = 100\n",
    "key, subkey = random.split(random.PRNGKey(0))\n",
    "init_state = ravel(*sample_from_prior(subkey, num_chains))\n",
    "\n",
    "@jit\n",
    "def run_chain(key, state):\n",
    "    kernel = tfp.mcmc.NoUTurnSampler(target_log_prob, 1e-8)\n",
    "#     kernel = tfp.mcmc.UncalibratedLangevin(target_log_prob, 1e-6)\n",
    "    return tfp.mcmc.sample_chain(1000,\n",
    "      current_state=state,\n",
    "      kernel=kernel,\n",
    "      trace_fn=None,\n",
    "      seed=key,\n",
    "      return_final_kernel_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/msc-thesis/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/mcmc/sample.py:327: UserWarning: supplied `TransitionKernel` is not calibrated. Markov chain may not converge to intended target distribution.\n",
      "  warnings.warn('supplied `TransitionKernel` is not calibrated. Markov '\n"
     ]
    }
   ],
   "source": [
    "key, subkey = random.split(key)\n",
    "sample_output = vmap(run_chain)(random.split(subkey, num_chains), init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logacc = sample_output.final_kernel_results.log_accept_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(np.exp(logacc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_output.all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = unravel(np.reshape(samples, newshape=(num_chains*50, 56)))\n",
    "s = unravel(samples[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa, asa = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_train_accuracy(wsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
