{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/lauro/code/msc-thesis/svgd/kernel_learning/\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random, lax, jacfwd, value_and_grad\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as onp\n",
    "import jax\n",
    "import pandas as pd\n",
    "    \n",
    "import utils\n",
    "import plot\n",
    "import distributions\n",
    "import models\n",
    "import flows\n",
    "from itertools import cycle, islice\n",
    "    \n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "sns.set(style='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpk = tfp.math.psd_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HHsy5lsf_S7"
   },
   "outputs": [],
   "source": [
    "covtype = datasets.fetch_covtype()\n",
    "features, labels = covtype['data'], covtype['target']\n",
    "\n",
    "num_features = features.shape[-1]\n",
    "num_classes = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, n_steps=500, batch_size=300):\n",
    "    \"\"\"Split x and y into batches\"\"\"\n",
    "    assert len(x) == len(y)\n",
    "    batch_cycle = cycle(zip(*[np.array_split(data, len(data)//batch_size) for data in (x, y)]))\n",
    "    return islice(batch_cycle, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num batches: 1549\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(x_train) // 300\n",
    "print(\"num batches:\", len(x_train) // 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 54)\n",
      "(301,) \n",
      "\n",
      "(301, 54)\n",
      "(301,) \n",
      "\n",
      "(301, 54)\n",
      "(301,) \n",
      "\n",
      "(301, 54)\n",
      "(301,) \n",
      "\n",
      "(301, 54)\n",
      "(301,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a, b in get_batches(x_train, y_train, 5):\n",
    "    print(a.shape)\n",
    "    print(b.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(x_train, y_train, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pXr2atnk8xA"
   },
   "source": [
    "We can define the model using `tfd.JointDistributionCoroutine`. We'll put standard normal priors on both the weights and the bias term then write a `target_log_prob` function that pins the sampled labels to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ri7RxnekWUr"
   },
   "outputs": [],
   "source": [
    "Root = tfd.JointDistributionCoroutine.Root\n",
    "\n",
    "def get_model(features_batch):\n",
    "    def model():\n",
    "        \"\"\"generator\"\"\"\n",
    "        w = yield Root(tfd.Sample(tfd.Normal(0., 1.), sample_shape=(num_features, num_classes), name=\"w\"))\n",
    "        b = yield Root(tfd.Sample(tfd.Normal(0., 1.), sample_shape=(num_classes,),              name=\"b\"))\n",
    "        logits = jnp.dot(features_batch, w) + b\n",
    "        _ = yield tfd.Independent(tfd.Categorical(logits=logits), reinterpreted_batch_ndims=1, name=\"labels\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_logp(x_batch, y_batch):\n",
    "    \"\"\"Stochastic estimate of the log-density (up to additive constant)\n",
    "    based on batch\"\"\"\n",
    "    def logp(params):\n",
    "        dist = tfd.JointDistributionCoroutine(get_model(x_batch))\n",
    "        return dist.log_prob(tuple(params) + (y_batch,))\n",
    "    return logp\n",
    "\n",
    "dist = tfd.JointDistributionCoroutine(get_model(x_train[:300]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have two distinct types of batching going on: we're using batches of data to estimate the gradient $\\nabla \\log p$, and then we're mapping that gradient over a set of particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24053.352\n",
      "-24053.352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([-51325.812 , -42315.832 , -19196.648 ,  -3738.0188,\n",
       "             -15562.641 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(batches)\n",
    "logp = get_logp(x, y)\n",
    "\n",
    "dist = tfd.JointDistributionCoroutine(get_model(x))\n",
    "*params, label = dist.sample(seed=key)\n",
    "print(dist.log_prob(params + [y]))\n",
    "print(logp(params))\n",
    "\n",
    "# now batched!\n",
    "*params, label = dist.sample(5, seed=key)\n",
    "vmap(logp)(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference using minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ravel (flatten) and unravel parameters like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dist.sample(seed=key)[:-1]\n",
    "params_flat, unravel = jax.flatten_util.ravel_pytree(params)\n",
    "# unravel(params_flat) == params\n",
    "# [a == b for a, b in zip(unravel(params_flat), params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_logp(x_batch, y_batch):\n",
    "    logp = get_logp(x_batch, y_batch)\n",
    "    def flat_logp(params_flat):\n",
    "        return logp(unravel(params_flat))\n",
    "    return flat_logp\n",
    "\n",
    "def ravel(params):\n",
    "    flat, _ = jax.flatten_util.ravel_pytree(params)\n",
    "    return flat\n",
    "\n",
    "def batch_ravel(batch):\n",
    "    return vmap(ravel)(batch)\n",
    "\n",
    "def batch_unravel(batch_flat):\n",
    "    return vmap(unravel)(batch_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lmc(key, init_batch):\n",
    "    \"\"\"init_batch is a batch of initial samples / particles.\"\"\"\n",
    "    particles = batch_ravel(init_batch)\n",
    "    eta = 1e-3\n",
    "    logps = []\n",
    "\n",
    "    @jit\n",
    "    def step(key, particles):\n",
    "        logp = get_flat_logp(x, y)\n",
    "        log_probs, grads = vmap(value_and_grad(logp))(particles)\n",
    "        particles += eta * grads + np.sqrt(2*eta) * random.normal(key, shape=particles.shape)\n",
    "        return particles, log_probs\n",
    "\n",
    "    for x, y in tqdm(get_batches(x_train, y_train, num_batches*2), total=num_batches*2):\n",
    "        key, subkey = random.split(key)\n",
    "        particles, log_probs = step(subkey, particles)\n",
    "        logps.append(log_probs)\n",
    "    return batch_unravel(particles), np.array(logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svgd(key, init_batch):\n",
    "    \"\"\"init_batch is a batch of initial samples / particles.\"\"\"\n",
    "    init_batch = batch_ravel(init_batch)\n",
    "    key, keya, keyb = random.split(key, 3)\n",
    "    kernel_gradient = models.KernelGradient(target_logp=logp, key=keya)\n",
    "    gradient = partial(kernel_gradient.gradient, scaled=True) # scale to match lambda_reg\n",
    "\n",
    "    svgd_particles = models.Particles(key=keyb,\n",
    "                                      gradient=gradient,\n",
    "                                      init_samples=init_batch,\n",
    "                                      learning_rate=1e-3,\n",
    "                                      num_groups=1)\n",
    "    for params, labels in get_batches(x_train, y_train, num_batches*2):\n",
    "        svgd_particles.step(None)\n",
    "    return batch_unravel(svgd_particles.particles.training), kernel_gradient, svgd_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neural_svgd(key, init_batch):\n",
    "    \"\"\"init_batch is a batch of initial samples / particles.\"\"\"\n",
    "    init_batch = batch_ravel(init_batch)\n",
    "    key, keya, keyb = random.split(key, 3)\n",
    "    learner = models.SDLearner(target_logp=logp, target_dim=init_batch.shape[1], key=keya)\n",
    "\n",
    "    particles = models.Particles(key=keyb,\n",
    "                                 gradient=learner.gradient,\n",
    "                                 init_samples=init_batch,\n",
    "                                 learning_rate=1e-3,\n",
    "                                 num_groups=2)\n",
    "    next_batch = partial(particles.next_batch, batch_size=None)\n",
    "    for x, y in get_batches(x_train, y_train, 1000):\n",
    "        key, subkey = random.split(key)\n",
    "        learner.train(next_batch, key=subkey, n_steps=1)\n",
    "        particles.step(learner.get_params())\n",
    "    return batch_unravel(particles.particles.training), learner, particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 263/3098 [00:03<00:38, 73.89it/s]"
     ]
    }
   ],
   "source": [
    "init_batch = dist.sample(500, seed=key)[:-1]\n",
    "lmc_samples, logps = run_lmc(key, init_batch)\n",
    "# svgd_samples, gradient, particles = run_svgd(key, init_batch)\n",
    "# neural_samples, neural_gradient, neural_particles = run_neural_svgd(key, init_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logps.mean(axis=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate samples on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tuple([p[0] for p in lmc_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logits\n",
    "test_dist = tfd.JointDistributionCoroutine(get_model(x_test))\n",
    "@jit\n",
    "def get_logits(params):\n",
    "    \"\"\"Returns logits shaped (n, 7), 7 being nr of categories\"\"\"\n",
    "    dists, _ = test_dist.sample_distributions(seed=random.PRNGKey(0), value=params + (None,))\n",
    "    logits = dists[-1].distribution.probs_parameter()\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dS01h9X3nBzh",
    "outputId": "320c6199-c773-4fac-e7ac-9af9ba858eb0"
   },
   "outputs": [],
   "source": [
    "# Parallel LMC (Lauro) samples\n",
    "all_probs = vmap(get_logits)(lmc_samples)\n",
    "print('Average accuracy:', jnp.mean(all_probs.argmax(axis=-1) == y_test))\n",
    "print('BMA accuracy:', jnp.mean(all_probs.mean(axis=0).argmax(axis=-1) == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dS01h9X3nBzh",
    "outputId": "320c6199-c773-4fac-e7ac-9af9ba858eb0"
   },
   "outputs": [],
   "source": [
    "# Random samples\n",
    "all_probs = vmap(get_logits)(tuple(dist.sample(1000, seed=key)[:-1]))\n",
    "print('Average accuracy:', jnp.mean(all_probs.argmax(axis=-1) == y_test))\n",
    "print('BMA accuracy:', jnp.mean(all_probs.mean(axis=0).argmax(axis=-1) == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.285714285714286"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100/7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
