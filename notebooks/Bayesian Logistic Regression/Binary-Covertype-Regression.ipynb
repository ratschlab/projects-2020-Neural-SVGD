{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b6423b542be2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhaiku\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/msc-thesis/svgd/kernel_learning/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunnel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbanana_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mring_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquiggle_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmix_of_gauss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolynomial_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/msc-thesis/svgd/kernel_learning/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;31m# rotated gaussian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0msubkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqmult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariances\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0mproposal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/msc-thesis/svgd/kernel_learning/utils.py\u001b[0m in \u001b[0;36mqmult\u001b[0;34m(key, b)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;31m# Apply the transformation to a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;31m# Tidy up signs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/ops/scatter.py\u001b[0m in \u001b[0;36mindex_update\u001b[0;34m(x, idx, y, indices_are_sorted, unique_indices)\u001b[0m\n\u001b[1;32m    300\u001b[0m          [1., 1., 1., 6., 6., 6.]], dtype=float32)\n\u001b[1;32m    301\u001b[0m   \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m   return _scatter_update(\n\u001b[0m\u001b[1;32m    303\u001b[0m       x, idx, y, lax.scatter, indices_are_sorted, unique_indices)\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/ops/scatter.py\u001b[0m in \u001b[0;36m_scatter_update\u001b[0;34m(x, idx, y, scatter_op, indices_are_sorted, unique_indices)\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# is more or less a transpose of the gather equivalent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_index_for_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,\n\u001b[0m\u001b[1;32m     51\u001b[0m                        indices_are_sorted, unique_indices)\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/ops/scatter.py\u001b[0m in \u001b[0;36m_scatter_impl\u001b[0;34m(x, y, scatter_op, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mscatter_dims_to_operand_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_index_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   )\n\u001b[0;32m---> 79\u001b[0;31m   out = scatter_op(x, indexer.gather_indices, y, dnums,\n\u001b[0m\u001b[1;32m     80\u001b[0m                    \u001b[0mindices_are_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices_are_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                    unique_indices=unique_indices)\n",
      "\u001b[0;32m~/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/lax/lax.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(operand, scatter_indices, updates, dimension_numbers, indices_are_sorted, unique_indices)\u001b[0m\n\u001b[1;32m   1025\u001b[0m   jaxpr, consts = _reduction_jaxpr(_scatter_reduction_computation,\n\u001b[1;32m   1026\u001b[0m                                    _abstractify(_const(operand, 0)))\n\u001b[0;32m-> 1027\u001b[0;31m   return scatter_p.bind(\n\u001b[0m\u001b[1;32m   1028\u001b[0m       \u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscatter_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_jaxpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m       \u001b[0mupdate_consts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_numbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension_numbers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mtop_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtop_trace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mxla_primitive_callable\u001b[0;34m(prim, *arg_specs, **params)\u001b[0m\n\u001b[1;32m    262\u001b[0m       device_assignment=device and (device.id,))\n\u001b[1;32m    263\u001b[0m   \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_is_tupled_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m   \u001b[0mcompiled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuilt_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnreps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_execute_compiled_primitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mbackend_compile\u001b[0;34m(backend, built_c, options)\u001b[0m\n\u001b[1;32m    323\u001b[0m   \u001b[0;31m# we use a separate function call to ensure that XLA compilation appears\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m   \u001b[0;31m# separately in Python profiling results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilt_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_execute_compiled_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/lauro/code/msc-thesis/svgd/kernel_learning/\")\n",
    "from jax import config\n",
    "config.update(\"jax_debug_nans\", False)\n",
    "from tqdm import tqdm\n",
    "from jax import config\n",
    "\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, random, lax, jacfwd, value_and_grad\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as onp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import haiku as hk\n",
    "    \n",
    "import utils\n",
    "import plot\n",
    "import distributions\n",
    "import stein\n",
    "import models\n",
    "import flows\n",
    "from itertools import cycle, islice\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from functools import partial\n",
    "import kernels\n",
    "import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "sns.set(style='white')\n",
    "\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpk = tfp.math.psd_kernels\n",
    "\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up exporting\n",
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "# save figures by using plt.savefig('title of figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('/home/lauro/code/msc-thesis/wang_svgd/data/covertype.mat')\n",
    "features = data['covtype'][:, 1:]\n",
    "features = onp.hstack([features, onp.ones([features.shape[0], 1])]) # add intercept term\n",
    "\n",
    "labels = data['covtype'][:, 0]\n",
    "labels[labels == 2] = 0\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "num_features = features.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_datapoints = len(x_train)\n",
    "num_batches = num_datapoints // batch_size\n",
    "\n",
    "\n",
    "def get_batches(x, y, n_steps=num_batches*2, batch_size=batch_size):\n",
    "    \"\"\"Split x and y into batches\"\"\"\n",
    "    assert len(x) == len(y)\n",
    "    assert x.ndim > y.ndim\n",
    "    n = len(x)\n",
    "    idxs = onp.random.choice(n, size=(n_steps, batch_size))\n",
    "    for idx in idxs:\n",
    "        yield x[idx], y[idx]\n",
    "#     batch_cycle = cycle(zip(*[onp.array_split(data, len(data)//batch_size) for data in (x, y)]))\n",
    "#     return islice(batch_cycle, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0, b0 = 1, 0.01 # hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy import stats, special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative model\n",
    "def sample_from_prior(key, num=100):\n",
    "    keya, keyb = random.split(key)\n",
    "    alpha = random.gamma(keya, a0, shape=(num,)) / b0\n",
    "    w = random.normal(keyb, shape=(num, num_features))\n",
    "    return w, np.log(alpha)\n",
    "\n",
    "\n",
    "def prior_logp(w, log_alpha):\n",
    "    \"\"\"\n",
    "    Returns logp(w, log_alpha) = sum_i(logp(wi, alphai))\n",
    "\n",
    "    w has shape (num_features,), or (n, num_features)\n",
    "    similarly, log_alpha may have shape () or (n,)\"\"\"\n",
    "    if log_alpha.ndim == 0:\n",
    "        assert w.ndim == 1\n",
    "    elif log_alpha.ndim == 1:\n",
    "        assert log_alpha.shape[0] == w.shape[0]\n",
    "\n",
    "    alpha = np.exp(log_alpha)\n",
    "    logp_alpha = np.sum(stats.gamma.logpdf(alpha, a0, scale=1/b0))\n",
    "    if w.ndim == 2:\n",
    "        logp_w = np.sum(vmap(lambda wi, alphai: stats.norm.logpdf(wi, scale=1/np.sqrt(alphai)))(w, alpha))\n",
    "    elif w.ndim == 1:\n",
    "        logp_w = np.sum(stats.norm.logpdf(w, scale=1/np.sqrt(alpha)))\n",
    "    else:\n",
    "        raise\n",
    "    return logp_alpha + logp_w\n",
    "\n",
    "\n",
    "def loglikelihood(y, x, w):\n",
    "    \"\"\"\n",
    "    compute log p(y | x, w) for a single parameter w of\n",
    "    shape (num_features,) and a batch of data (y, x) of\n",
    "    shape (m,) and (m, num_features)\n",
    "\n",
    "    log p(y | x, w) = sum_i(logp(yi| xi, w))\n",
    "    \"\"\"\n",
    "    y = ((y - 1/2)*2).astype(np.int32)\n",
    "    logits = x @ w\n",
    "    prob_y = special.expit(logits*y)\n",
    "    return np.sum(np.log(prob_y))\n",
    "\n",
    "\n",
    "def log_posterior_unnormalized(y, x, w, log_alpha):\n",
    "    \"\"\"All is batched\"\"\"\n",
    "    log_prior = prior_logp(w, log_alpha)\n",
    "    log_likelihood = np.sum(vmap(lambda wi: loglikelihood(y, x, wi))(w))\n",
    "    return log_prior + log_likelihood\n",
    "\n",
    "\n",
    "def log_posterior_unnormalized_single_param(y, x, w, log_alpha):\n",
    "    \"\"\"y, x are batched, w, log_alpha not. In case I need\n",
    "    an unbatched eval of the target logp.\"\"\"\n",
    "    log_prior = prior_logp(w, log_alpha)\n",
    "    log_likelihood = loglikelihood(y, x, w)\n",
    "    return log_prior + log_likelihood\n",
    "\n",
    "\n",
    "def compute_probs(y, x, w):\n",
    "    \"\"\"y and x are data batches. w is a single parameter\n",
    "    array of shape (num_features,)\"\"\"\n",
    "    y = ((y - 1/2)*2).astype(np.int32)\n",
    "    logits = x @ w\n",
    "    prob_y = special.expit(logits*y)\n",
    "    return prob_y\n",
    "\n",
    "\n",
    "@jit\n",
    "def compute_test_accuracy(w):\n",
    "    probs = vmap(lambda wi: compute_probs(y_test, x_test, wi))(w)\n",
    "    probs_y = np.mean(probs, axis=0)\n",
    "    return np.mean(probs_y > 0.5)\n",
    "\n",
    "\n",
    "@jit\n",
    "def compute_train_accuracy(w):\n",
    "    probs = vmap(lambda wi: compute_probs(y_train, x_train, wi))(w)\n",
    "    probs_y = np.mean(probs, axis=0)\n",
    "    return np.mean(probs_y > 0.5)\n",
    "\n",
    "\n",
    "def ravel(w, log_alpha):\n",
    "    return np.hstack([w, np.expand_dims(log_alpha, -1)])\n",
    "\n",
    "\n",
    "def unravel(params):\n",
    "    if params.ndim == 1:\n",
    "        return params[:-1], params[-1]\n",
    "    elif params.ndim == 2:\n",
    "        return params[:, :-1], np.squeeze(params[:, -1])\n",
    "\n",
    "\n",
    "def get_minibatch_logp(x, y):\n",
    "    \"\"\"\n",
    "    Returns callable logp that computes the unnormalized target\n",
    "    log pdf of raveled (flat) params with shape (num_features+1,)\n",
    "    or shape (n, num_features+1).\n",
    "\n",
    "    y, x are minibatches of data.\"\"\"\n",
    "    assert len(x) == len(y)\n",
    "    assert x.ndim > y.ndim\n",
    "\n",
    "    def logp(params): # TODO: if this doesn't work, then modify to just take a single param vector\n",
    "        \"\"\"params = ravel(w, log_alpha)\"\"\"\n",
    "        w, log_alpha = unravel(params)\n",
    "        log_prior = prior_logp(w, log_alpha)\n",
    "        if w.ndim == 1:\n",
    "            mean_loglikelihood = loglikelihood(y, x, w)\n",
    "        elif w.ndim == 2:\n",
    "            mean_loglikelihood = np.mean(vmap(lambda wi: loglikelihood(y, x, wi))(w))\n",
    "        else:\n",
    "            raise\n",
    "        return log_prior + num_datapoints * mean_loglikelihood # = grad(log p)(theta) + N/n sum_i grad(log p)(theta | x)\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "w, log_alpha = sample_from_prior(subkey, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = x_train[:100]\n",
    "ys = y_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_posterior_unnormalized(ys, xs, w, log_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_test_accuracy(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = get_minibatch_logp(xs, ys)\n",
    "params = ravel(w, log_alpha)\n",
    "lp(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "NUM_VALS = 5*NUM_EPOCHS # number of test accuracy evaluations per run\n",
    "NUM_STEPS = num_batches*NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tv(key):\n",
    "    return ravel(*sample_from_prior(key, num=batch_size)).split(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svgd(key, lr):\n",
    "    key, subkey = random.split(key)\n",
    "    init_particles = ravel(*sample_from_prior(subkey, 100))\n",
    "#     svgd_opt = optax.chain(optax.scale_by_schedule(utils.polynomial_schedule),\n",
    "#                            optax.scale_by_rms(),\n",
    "#                            optax.scale(-lr))\n",
    "    svgd_opt = optax.sgd(lr)\n",
    "\n",
    "    svgd_grad = models.KernelGradient(get_target_logp=lambda batch: get_minibatch_logp(*batch), scaled=False)\n",
    "    particles = models.Particles(key, svgd_grad.gradient, init_particles, custom_optimizer=svgd_opt)\n",
    "\n",
    "    test_batches = get_batches(x_test, y_test, 2*NUM_VALS, batch_size=batch_size)\n",
    "    train_batches = get_batches(x_train, y_train, NUM_STEPS+1)\n",
    "    for i, batch in tqdm(enumerate(train_batches), total=NUM_STEPS):\n",
    "        particles.step(batch)\n",
    "        if i % (NUM_STEPS//NUM_VALS) == 0:\n",
    "            test_logp = get_minibatch_logp(*next(test_batches))\n",
    "            stepdata = {\n",
    "                \"accuracy\": compute_test_accuracy(unravel(particles.particles.training)[0]),\n",
    "                \"test_logp\": test_logp(particles.particles.training),\n",
    "            }\n",
    "            metrics.append_to_log(particles.rundata, stepdata)\n",
    "\n",
    "    particles.done()\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neural_svgd(key, lr):\n",
    "    \"\"\"init_batch is a batch of initial samples / particles.\n",
    "    Note: there's two types of things I call 'batch': a batch from the dataset\n",
    "    and a batch of particles. don't confuse them\"\"\"\n",
    "    key, subkey = random.split(key)\n",
    "    init_particles = ravel(*sample_from_prior(subkey, batch_size))\n",
    "    nsvgd_opt = optax.sgd(lr)\n",
    "\n",
    "    key1, key2 = random.split(key)\n",
    "    neural_grad = models.SDLearner(target_dim=init_particles.shape[1],\n",
    "                                   get_target_logp=lambda batch: get_minibatch_logp(*batch),\n",
    "                                   learning_rate=5e-3,\n",
    "                                   key=key1,\n",
    "                                   aux=False)\n",
    "    particles = models.Particles(key2, neural_grad.gradient, init_particles, custom_optimizer=nsvgd_opt)\n",
    "\n",
    "    # Warmup on first batch\n",
    "    neural_grad.train(next_batch=sample_tv,\n",
    "                      n_steps=100,\n",
    "                      early_stopping=False,\n",
    "                      data=next(get_batches(x_train, y_train, 2)))\n",
    "\n",
    "    next_particles = partial(particles.next_batch)\n",
    "    test_batches = get_batches(x_test, y_test, 2*NUM_VALS, batch_size=batch_size)\n",
    "    train_batches = get_batches(x_train, y_train, NUM_STEPS+1)\n",
    "    for i, data_batch in tqdm(enumerate(train_batches), total=NUM_STEPS):\n",
    "        neural_grad.train(next_batch=next_particles, n_steps=10, data=data_batch)\n",
    "        particles.step(neural_grad.get_params())\n",
    "        if i % (NUM_STEPS//NUM_VALS)==0:\n",
    "            test_logp = get_minibatch_logp(*next(test_batches))\n",
    "            train_logp = get_minibatch_logp(*data_batch)\n",
    "            stepdata = {\n",
    "                \"accuracy\": compute_test_accuracy(unravel(particles.particles.training)[0]),\n",
    "                \"test_logp\": test_logp(particles.particles.training),\n",
    "                \"training_logp\": train_logp(particles.particles.training),\n",
    "            }\n",
    "            metrics.append_to_log(particles.rundata, stepdata)\n",
    "    neural_grad.done()\n",
    "    particles.done()\n",
    "    return particles, neural_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = utils.polynomial_schedule\n",
    "\n",
    "def run_sgld(key, lr):\n",
    "    key, subkey = random.split(key)\n",
    "    init_particles = ravel(*sample_from_prior(subkey, 100))\n",
    "    \"\"\"init_batch = (w, log_alpha) is a batch of initial samples / particles.\"\"\"\n",
    "    key, subkey = random.split(key)\n",
    "#     sgld_opt = utils.scaled_sgld(subkey, lr, schedule)\n",
    "    sgld_opt = utils.sgld(lr, 0)\n",
    "\n",
    "    def energy_gradient(data, particles, aux=True):\n",
    "        \"\"\"data = [batch_x, batch_y]\"\"\"\n",
    "        xx, yy = data\n",
    "        logp = get_minibatch_logp(xx, yy)\n",
    "        logprob, grads = value_and_grad(logp)(particles)\n",
    "        if aux:\n",
    "            return -grads, {\"logp\": logprob}\n",
    "        else:\n",
    "            return -grads\n",
    "\n",
    "    particles = models.Particles(key, energy_gradient, init_particles, custom_optimizer=sgld_opt)\n",
    "    test_batches = get_batches(x_test, y_test)\n",
    "    train_batches = get_batches(x_train, y_train, NUM_STEPS+1)\n",
    "    for i, batch_xy in tqdm(enumerate(train_batches), total=NUM_STEPS):\n",
    "        particles.step(batch_xy)\n",
    "        if i % (NUM_STEPS//NUM_VALS)==0:\n",
    "            test_logp = get_minibatch_logp(*next(test_batches))\n",
    "            stepdata = {\n",
    "                \"accuracy\": compute_test_accuracy(unravel(particles.particles.training)[0]),\n",
    "                \"train_accuracy\": compute_train_accuracy(unravel(particles.particles.training)[0]),\n",
    "                \"test_logp\": np.mean(test_logp(particles.particles.training))\n",
    "            }\n",
    "            metrics.append_to_log(particles.rundata, stepdata)\n",
    "    particles.done()\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run samplers\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "sgld_p = run_sgld(subkey, 1e-6)\n",
    "# svgd_p = run_svgd(subkey, 5e-2)\n",
    "# neural_p, neural_grad = run_neural_svgd(subkey, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgld_aux = sgld_p.rundata\n",
    "svgd_aux = svgd_p.rundata\n",
    "neural_aux = neural_p.rundata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate samples on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgld_accs, svgd_accs, neural_accs = [aux[\"accuracy\"] for aux in (sgld_aux, svgd_aux, neural_aux)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=[15, 8])\n",
    "names = [\"SGLD\", \"SVGD\", \"Neural\"]\n",
    "accs = [sgld_accs, svgd_accs, neural_accs]\n",
    "for name, acc in zip(names, accs):\n",
    "    plt.plot(acc, \"--.\", label=name)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaced_idx = np.arange(0, NUM_STEPS, NUM_STEPS // NUM_VALS)\n",
    "plt.plot(sgld_aux[\"training_logp\"])\n",
    "plt.plot(spaced_idx, sgld_aux[\"test_logp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaced_idx = np.arange(0, NUM_STEPS, NUM_STEPS // NUM_VALS)\n",
    "plt.plot(neural_aux[\"training_logp\"])\n",
    "plt.plot(spaced_idx, neural_aux[\"test_logp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "def sgld_acc(lr):\n",
    "    particles = run_sgld(subkey, lr)\n",
    "    acc = particles.rundata[\"accuracy\"]\n",
    "    return np.mean(np.array(acc[-10:]))\n",
    "\n",
    "def svgd_acc(lr):\n",
    "    particles = run_svgd(subkey, lr)\n",
    "    acc = particles.rundata[\"accuracy\"]\n",
    "    return np.mean(np.array(acc[-10:]))\n",
    "\n",
    "def nsvgd_acc(lr):\n",
    "    particles, _ = run_neural_svgd(subkey, lr)\n",
    "    acc = particles.rundata[\"accuracy\"]\n",
    "    return np.mean(np.array(acc[-10:]))\n",
    "\n",
    "def print_accs(lrs, accs):\n",
    "    accs = np.asarray(accs)\n",
    "    print(accs)\n",
    "    print(np.argmax(accs))\n",
    "    plt.plot(lrs, accs, \"--.\")\n",
    "    plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "lrs = np.logspace(-9, -4, 15)\n",
    "for lr in lrs:\n",
    "    accs.append(sgld_acc(lr))\n",
    "accs = np.array(accs)\n",
    "print_accs(lrs, accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "lrs = np.logspace(-5, -1, 15)\n",
    "for lr in lrs:\n",
    "    accs.append(svgd_acc(lr))\n",
    "accs = np.array(accs)\n",
    "print_accs(lrs, accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "lrs = np.logspace(-5, -1, 15)\n",
    "for lr in lrs:\n",
    "    accs.append(nsvgd_acc(lr))\n",
    "    print(accs[-1])\n",
    "accs = np.array(accs)\n",
    "print_accs(lrs, accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural rundata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=[15, 8])\n",
    "plt.plot(sgld_aux[\"training_mean\"], \"--o\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neural_grad.rundata[\"train_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "plt.subplots(figsize=[15, 8])\n",
    "plt.plot(neural_grad.rundata[\"training_loss\"])\n",
    "plt.plot(neural_grad.rundata[\"validation_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=[15, 8])\n",
    "plt.plot(neural_aux[\"training_mean\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=[15, 8])\n",
    "plt.plot(svgd_aux[\"training_mean\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calibration curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_probs(params):\n",
    "    \"\"\"Returns test probabilities P(y=1) for\n",
    "    all y in the test set, for w a parameter array\n",
    "    of shape (n, num_features)\"\"\"\n",
    "    w, _ = unravel(params)\n",
    "    probs = vmap(lambda wi: compute_probs(y_test, x_test, wi))(w)\n",
    "    return np.mean(probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = [batch_probs(p.particles.training) for p in (sgld_p, svgd_p, neural_p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=[17, 5])\n",
    "\n",
    "for ax, probs, name in zip(axs, probabilities, names):\n",
    "    true_freqs, bins = calibration_curve(y_test, probs, n_bins=10)\n",
    "    ax.plot(true_freqs, bins, \"--o\")\n",
    "#     print(bins)\n",
    "    ax.plot(bins, bins)\n",
    "    ax.set_ylabel(\"True frequency\")\n",
    "    ax.set_xlabel(\"Predicted probability\")\n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot accuracy vs. certainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdlfk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "certainty = np.max([1 - probs, probs])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
