{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a BNN to classify MNIST using neural SVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Train a Bayesian neural network to classify MNIST using\n",
    "# Neural SVGD\n",
    "#\n",
    "# If using pmap, set the environment variable\n",
    "# `export XLA_FLAGS=\"--xla_force_host_platform_device_count=8\"`\n",
    "# before running on CPU (this enables pmap to \"see\" multiple cores).\n",
    "import sys\n",
    "sys.path.append(\"../../learning_particle_gradients/\")\n",
    "sys.path.append(\"../../experiments/\")\n",
    "from functools import partial\n",
    "from itertools import cycle\n",
    "\n",
    "import numpy as onp\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import jit, grad, value_and_grad, vmap, pmap, config, random\n",
    "config.update(\"jax_debug_nans\", False)\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "import nets\n",
    "import utils\n",
    "import models\n",
    "from convnet import model, accuracy, crossentropy_loss, log_prior\n",
    "\n",
    "# Config\n",
    "key = random.PRNGKey(0)\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 1e-8\n",
    "META_LEARNING_RATE = 1e-4\n",
    "NUM_SAMPLES = 4\n",
    "DISABLE_PROGRESS_BAR = False\n",
    "USE_PMAP = False\n",
    "\n",
    "if USE_PMAP:\n",
    "    vpmap = pmap\n",
    "else:\n",
    "    vpmap = vmap\n",
    "\n",
    "# Load MNIST\n",
    "data_dir = '/tmp/tfds'\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
    "mnist_data = tfds.as_numpy(mnist_data)\n",
    "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
    "\n",
    "# Full train and test set\n",
    "train_images, train_labels = train_data['image'], train_data['label']\n",
    "test_images, test_labels = test_data['image'], test_data['label']\n",
    "\n",
    "# Split off the validation set\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.1, random_state=0)\n",
    "data_size = len(train_images)\n",
    "\n",
    "\n",
    "def make_batches(images, labels, batch_size):\n",
    "    \"\"\"Returns an iterator that cycles through \n",
    "    tuples (image_batch, label_batch).\"\"\"\n",
    "    num_batches = len(images) // batch_size\n",
    "    split_idx = onp.arange(1, num_batches+1)*batch_size\n",
    "    batches = zip(*[onp.split(data, split_idx, axis=0) for data in (images, labels)])\n",
    "    return cycle(batches)\n",
    "\n",
    "\n",
    "def loss(params, images, labels):\n",
    "    \"\"\"Minibatch approximation of the (unnormalized) Bayesian\n",
    "    negative log-posterior evaluated at `params`. That is,\n",
    "    -log model_likelihood(data_batch | params) * batch_rescaling_constant - log prior(params))\"\"\"\n",
    "    logits = model.apply(params, images)\n",
    "    return data_size/BATCH_SIZE * crossentropy_loss(logits, labels) -  log_prior(params) \n",
    "\n",
    "\n",
    "@jit\n",
    "def ensemble_accuracy(param_set):\n",
    "    \"\"\"use ensemble predictions to compute validation accuracy\"\"\"\n",
    "    vapply = vpmap(model.apply, (0, None))\n",
    "    logits = vapply(param_set, val_images[:BATCH_SIZE])\n",
    "    preds = jnp.mean(vmap(jax.nn.softmax)(logits), axis=0) # mean prediction\n",
    "    return jnp.mean(preds.argmax(axis=1) == val_labels[:BATCH_SIZE])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural SVGD Model\n",
    "* Input: model parameters\n",
    "* Output: 'gradient' of same shape as parameters\n",
    "\n",
    "### Memory\n",
    "Assume: BNN model has 50.000 parameters, and NSVGD layers have hidden dimensions `[1024, 1024]`. Then the NSVGD (meta) model has `2*50.000*1024` parameters (each 8 bytes), i.e. about 800MB.\n",
    "\n",
    "The activations: `50.000 + 1024 + 1024 + 50.000` floats, i.e. about 800KB, times `NUM_SAMPLES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model parameters so we know their shape\n",
    "key, subkey = random.split(key)\n",
    "params_tree = model.init(subkey, train_images[:2])\n",
    "params_flat, unravel = jax.flatten_util.ravel_pytree(params_tree)\n",
    "\n",
    "# def model_fn(params):\n",
    "#     mlp = nets.MLP([1024, 1024, params_flat.shape[0]])\n",
    "#     scale = hk.get_parameter(\"scale\", (), init=lambda *args: jnp.ones(*args))\n",
    "#     return scale*mlp(params)\n",
    "\n",
    "# dynamics_model = hk.without_apply_rng(hk.transform(model_fn))\n",
    "# dyn_params = dynamics_model.init(subkey, params_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Neural SVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ravel(tree):\n",
    "    return jax.flatten_util.ravel_pytree(tree)[0]\n",
    "\n",
    "\n",
    "def init_flat_params(key):\n",
    "    return ravel(model.init(key, train_images[:2]))\n",
    "\n",
    "\n",
    "def get_minibatch_loss(batch):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        batch = (images, labels)\n",
    "\n",
    "    Returns a callable that computes target posterior\n",
    "    given flattened param vector.\n",
    "    \"\"\"\n",
    "    def minibatch_loss(params_flat):\n",
    "        return loss(unravel(params_flat), *batch)\n",
    "    return minibatch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "init_particles = vmap(init_flat_params)(random.split(subkey, NUM_SAMPLES))\n",
    "\n",
    "opt = optax.sgd(LEARNING_RATE)\n",
    "# opt_state = opt.init(init_particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1, key2 = random.split(key)\n",
    "neural_grad = models.SDLearner(target_dim=init_particles.shape[1],\n",
    "                               get_target_logp=get_minibatch_loss,\n",
    "                               learning_rate=META_LEARNING_RATE,\n",
    "                               key=key1,\n",
    "                               sizes=[1024, 1024, init_particles.shape[1]],\n",
    "                               aux=False)\n",
    "particles = models.Particles(key2, neural_grad.gradient, init_particles, custom_optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tv(key):\n",
    "    return vmap(init_flat_params)(random.split(subkey, NUM_SAMPLES)).split(2)\n",
    "\n",
    "batches = make_batches(train_images, train_labels, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neural_grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-989f44d878ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Warmup on first batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m neural_grad.train(next_batch=sample_tv,\n\u001b[0m\u001b[1;32m      3\u001b[0m                   \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   data=next(batches))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'neural_grad' is not defined"
     ]
    }
   ],
   "source": [
    "# Warmup on first batch\n",
    "neural_grad.train(next_batch=sample_tv,\n",
    "                  n_steps=3, # 100\n",
    "                  early_stopping=False,\n",
    "                  data=next(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_particles = partial(particles.next_batch)\n",
    "test_batches = get_batches(x_test, y_test, 2*NUM_VALS) if full_data else get_batches(x_val, y_val, 2*NUM_VALS)\n",
    "train_batches = get_batches(xx, yy, NUM_STEPS+1) if full_data else get_batches(x_train, y_train, NUM_STEPS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data_batch in tqdm(enumerate(train_batches), total=NUM_STEPS, disable=not progress_bar):\n",
    "    neural_grad.train(next_batch=next_particles, n_steps=10, data=data_batch)\n",
    "    particles.step(neural_grad.get_params())\n",
    "    if i % (NUM_STEPS//NUM_VALS)==0:\n",
    "        test_logp = get_minibatch_logp(*next(test_batches))\n",
    "        train_logp = get_minibatch_logp(*data_batch)\n",
    "        stepdata = {\n",
    "            \"accuracy\": compute_test_accuracy(unravel(particles.particles.training)[0]),\n",
    "            \"test_logp\": test_logp(particles.particles.training),\n",
    "            \"training_logp\": train_logp(particles.particles.training),\n",
    "        }\n",
    "        metrics.append_to_log(particles.rundata, stepdata)\n",
    "neural_grad.done()\n",
    "particles.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@jit\n",
    "def step(param_set, opt_state, images, labels):\n",
    "    \"\"\"Update param_set elements in parallel using Langevin dynamics.\"\"\"\n",
    "    step_losses, g = vpmap(value_and_grad(loss), (0, None, None))(param_set, images, labels)\n",
    "    g, opt_state = opt.update(g, opt_state, param_set)\n",
    "    return optax.apply_updates(param_set, g), opt_state, step_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize set of parameters\n",
    "key, subkey = random.split(key)\n",
    "param_set = vmap(model.init, (0, None))(random.split(subkey, NUM_SAMPLES), train_images[:5])\n",
    "opt_state = opt.init(param_set)\n",
    "\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "batches = make_batches(train_images, train_labels, BATCH_SIZE)\n",
    "n_train_steps = EPOCHS * data_size // BATCH_SIZE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
