{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a BNN to classify MNIST using neural SVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for leonhard\n",
    "import os\n",
    "# os.environ['XLA_FLAGS'] = \"--xla_gpu_cuda_data_dir=\" + os.environ[\"CUDA_HOME\"]\n",
    "# os.environ['XLA_FLAGS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Train a Bayesian neural network to classify MNIST using\n",
    "# Neural SVGD\n",
    "#\n",
    "# If using pmap, set the environment variable\n",
    "# `export XLA_FLAGS=\"--xla_force_host_platform_device_count=8\"`\n",
    "# before running on CPU (this enables pmap to \"see\" multiple cores).\n",
    "%load_ext autoreload\n",
    "import sys\n",
    "import os\n",
    "on_cluster = not os.getenv(\"HOME\") == \"/home/lauro\"\n",
    "if on_cluster:\n",
    "    sys.path.append(\"/cluster/home/dlauro/projects-2020-Neural-SVGD/learning_particle_gradients/\")\n",
    "sys.path.append(\"../../experiments/\")\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import vmap, random\n",
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "import bnn\n",
    "import models\n",
    "import metrics\n",
    "import mnist\n",
    "import config as cfg\n",
    "from jax import jit, grad\n",
    "\n",
    "# Config\n",
    "key = random.PRNGKey(0)\n",
    "MAX_TRAIN_STEPS = 100  # df 100\n",
    "META_LEARNING_RATE = 1e-3  # df 1e-3; should be as high as possible; regularize w/ max steps\n",
    "PATIENCE = 5  # df 5; early stopping not v helpful, bc we overfit on all ps\n",
    "\n",
    "NUM_SAMPLES = 100\n",
    "DISABLE_PROGRESS_BAR = False\n",
    "USE_PMAP = False\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LAMBDA_REG = 10**2\n",
    "STEP_SIZE = 1e-3 #1e-3, 1e-4 too large (w adam)\n",
    "PATIENCE = 5\n",
    "MAX_TRAIN_STEPS = 100 #15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural SVGD Model\n",
    "* Input: model parameters\n",
    "* Output: 'gradient' of same shape as parameters\n",
    "\n",
    "### Memory\n",
    "Assume: convnet model has 50.000 parameters, and NSVGD layers have hidden dimensions `[1024, 1024]`. Then the NSVGD (meta) model has `2*50.000*1024` parameters (each 8 bytes), i.e. about 800MB.\n",
    "\n",
    "The activations: `50.000 + 1024 + 1024 + 50.000` floats, i.e. about 800KB, times `NUM_SAMPLES`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init particles and dynamics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "init_particles = vmap(bnn.init_flat_params)(random.split(subkey, NUM_SAMPLES))\n",
    "opt = optax.sgd(STEP_SIZE)\n",
    "\n",
    "# opt = optax.chain(\n",
    "#     optax.scale_by_adam(),\n",
    "#     optax.scale(-STEP_SIZE),\n",
    "# )\n",
    "\n",
    "key, subkey1, subkey2 = random.split(key, 3)\n",
    "neural_grad = models.SDLearner(target_dim=init_particles.shape[1],\n",
    "                               get_target_logp=bnn.get_minibatch_logp,\n",
    "                               learning_rate=META_LEARNING_RATE,\n",
    "                               key=subkey1,\n",
    "                               sizes=[64, 64, init_particles.shape[1]],\n",
    "                               aux=False,\n",
    "                               use_hutchinson=True,\n",
    "                               lambda_reg=LAMBDA_REG,\n",
    "                               patience=PATIENCE,\n",
    "                               dropout=True)\n",
    "particles = models.Particles(subkey2, neural_grad.gradient, init_particles, custom_optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 12160)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_particles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytical sd: 137514460.0\n",
      "hutchinson estimate: 137565340.0\n",
      "ratio: 0.99963015\n"
     ]
    }
   ],
   "source": [
    "# maximal initial stein discrepancy (theoretical ideal)\n",
    "import plot\n",
    "import stein\n",
    "from jax.scipy import stats\n",
    "\n",
    "n, d = init_particles.shape\n",
    "n = 100\n",
    "key, subkey = random.split(key)\n",
    "xs = random.normal(subkey, (n, d)) / 100\n",
    "\n",
    "first_batch = next(mnist.training_batches)\n",
    "images, labels = first_batch\n",
    "\n",
    "def logp(x):\n",
    "    def loglikelihood(x):\n",
    "        params = bnn.unravel(x)\n",
    "        logits = bnn.model.apply(params, images)\n",
    "        return -mnist.train_data_size/cfg.batch_size * bnn.crossentropy_loss(logits, labels)\n",
    "    return loglikelihood(x) + bnn.log_prior(bnn.unravel(x))\n",
    "\n",
    "\n",
    "def logq(x):\n",
    "    return stats.norm.logpdf(x, loc=0, scale=1/100).sum()\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    def loglikelihood(x):\n",
    "        params = bnn.unravel(x)\n",
    "        logits = bnn.model.apply(params, images)\n",
    "        return -mnist.train_data_size/cfg.batch_size * bnn.crossentropy_loss(logits, labels) / (2*LAMBDA_REG)\n",
    "    return grad(loglikelihood)(x)\n",
    "\n",
    "# double-check stein discrepancy\n",
    "\n",
    "# a) true\n",
    "l2 = utils.l2_norm_squared(xs, f) # = sd(f*) / (2 LAMBDA_REG)\n",
    "true_sd = 2 * LAMBDA_REG * l2\n",
    "min_loss = -l2 * LAMBDA_REG\n",
    "\n",
    "# b) hutchinson\n",
    "key, subkey = random.split(key)\n",
    "random_estimate_sd = stein.stein_discrepancy_hutchinson(subkey, xs, logp, f)\n",
    "\n",
    "print(\"analytical sd:\", true_sd)\n",
    "print(\"hutchinson estimate:\", random_estimate_sd)\n",
    "print(\"ratio:\", true_sd / random_estimate_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Warmup on first batch\n",
    "# NUM_WARMUP_ITER = 5\n",
    "# key, subkey = random.split(key)\n",
    "# neural_grad.warmup(key=subkey,\n",
    "#                    sample_split_particles=sample_tv,\n",
    "#                    next_data=lambda: first_batch,\n",
    "#                    n_iter=NUM_WARMUP_ITER,\n",
    "#                    n_inner_steps=100,\n",
    "#                    progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=[15, 5])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(neural_grad.rundata['training_loss'])\n",
    "ax.plot(neural_grad.rundata['validation_loss'])\n",
    "ax.axhline(y=min_loss, label=\"true first-batch loss\", linestyle=\"--\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(neural_grad.rundata['training_sd'])\n",
    "ax.plot(neural_grad.rundata['validation_sd'])\n",
    "ax.axhline(y=true_sd, linestyle=\"--\", label=\"true first-batch stein discrepancy\")\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(neural_grad.rundata['l2_norm'])\n",
    "ax.axhline(y=l2, linestyle=\"--\", label=\"true l2 norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_grad.rundata['train_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_PARTICLE_STEPS = 5\n",
    "\n",
    "key, fixed_key = random.split(key)\n",
    "def step(key, train_batch):\n",
    "    \"\"\"one iteration of the particle trajectory simulation\"\"\"\n",
    "    neural_grad.train(\n",
    "        split_particles=particles.next_batch(key),\n",
    "        n_steps=MAX_TRAIN_STEPS,\n",
    "        data=train_batch\n",
    "    )\n",
    "    particles.step(neural_grad.get_params())\n",
    "\n",
    "\n",
    "@jit\n",
    "def compute_eval(train_batch, ps):\n",
    "    test_logp = utils.vmean(bnn.get_minibatch_logp(next(mnist.test_batches)))\n",
    "    train_logp = utils.vmean(bnn.get_minibatch_logp(train_batch))\n",
    "    stepdata = {\n",
    "        \"accuracy\": bnn.compute_acc_from_flat(ps),\n",
    "        \"test_logp\": test_logp(ps),\n",
    "        \"training_logp\": train_logp(ps),\n",
    "        \"particle_mean\": ps.mean(),\n",
    "    }\n",
    "    return stepdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* currently testing what happens if we **fix** train/val splits\n",
    "    * outcome: no overfit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_steps = EPOCHS * data_size // BATCH_SIZE // 5\n",
    "num_steps = 101\n",
    "for step_counter in tqdm(range(num_steps)):\n",
    "    key, subkey = random.split(key)\n",
    "    train_batch = next(mnist.training_batches)\n",
    "    step(subkey, train_batch)\n",
    "#     if step_counter % (num_steps//NUM_VALS) == 0:\n",
    "    if step_counter % 2 == 0:\n",
    "        metrics.append_to_log(particles.rundata,\n",
    "                              compute_eval(train_batch, particles.particles))\n",
    "# neural_grad.done()\n",
    "# particles.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=[15, 5])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(particles.rundata[\"accuracy\"], \"--.\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(neural_grad.rundata['training_loss'], \".\", label=\"training loss\")\n",
    "ax.plot(neural_grad.rundata['validation_loss'], \".\", label=\"validation loss\")\n",
    "ax.axhline(y=min_loss, label=\"loss should never go below here\", linestyle=\"--\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(particles.rundata['training_logp'])\n",
    "# ax.plot(particles.rundata['test_logp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=[15, 5])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(particles.rundata[\"accuracy\"], \"--.\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(neural_grad.rundata['training_loss'], \".\", label=\"training loss\")\n",
    "ax.plot(neural_grad.rundata['validation_loss'], \".\", label=\"validation loss\")\n",
    "ax.axhline(y=min_loss, label=\"loss should never go below here\", linestyle=\"--\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(particles.rundata['training_logp'])\n",
    "# ax.plot(particles.rundata['test_logp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onp.mean(particles.rundata[\"accuracy\"][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neural_grad.rundata[\"global_gradient_norm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles.rundata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = onp.array(particles.rundata['particles'])\n",
    "trajectories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize trajectory avg across dimensions (distinguish particles)\n",
    "fig, axs = plt.subplots(2, 1, figsize=[10, 8])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(trajectories.mean(axis=2));  # avg across dims\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(trajectories[:, :, 1]);  # watch single param (aka single dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize trajectory avg across particles (distinguish dims, ie parameters)\n",
    "fig, axs = plt.subplots(2, 1, figsize=[10, 8])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(trajectories.mean(axis=1)); # avg across particles\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(trajectories[:, 11, :]); # watch single particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate markings based on num_steps\n",
    "plt.subplots(figsize=[15, 5])\n",
    "markings = onp.cumsum(neural_grad.rundata['train_steps'])[NUM_WARMUP_ITER:]\n",
    "\n",
    "plt.plot(neural_grad.rundata['training_loss'])\n",
    "plt.plot(neural_grad.rundata['validation_loss'])\n",
    "plt.scatter(markings, onp.zeros(len(markings)))\n",
    "plt.axhline(y=min_loss, label=\"loss should never go below here\", linestyle=\"--\", color=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 800\n",
    "b = 1000\n",
    "marker_a = (markings < a).sum()\n",
    "marker_b = (markings < b).sum()\n",
    "print(f\"step {marker_a} to step {marker_b}.\")\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=[10, 10])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(neural_grad.rundata['training_loss'][a:b], \".\")\n",
    "ax.plot(neural_grad.rundata['validation_loss'][a:b], \".\")\n",
    "ax.scatter(markings[marker_a:marker_b] - a, onp.zeros(marker_b - marker_a))\n",
    "ax.axhline(y=min_loss, label=\"loss should never go below here\", linestyle=\"--\", color=\"g\")\n",
    "ax.legend()\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(neural_grad.rundata['l2_norm'][a:b])\n",
    "ax.scatter(markings[marker_a:marker_b] - a, onp.zeros(marker_b - marker_a))\n",
    "# ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# particle gradient norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = neural_grad.grads(particles.particles)\n",
    "optax.global_norm(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"norm pre upate :\", particles.rundata['global_grad_norm'][0])\n",
    "print(\"norm post upate:\", particles.rundata['global_grad_norm_post_update'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(particles.rundata['global_grad_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(particles.rundata['global_grad_norm_post_update'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optax.global_norm(vmap(unravel)(particles.particles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.sqrt(jnp.sum(particles.particles**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model gradient norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interlude: why accuracy no change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = vmap(lambda ps: model.apply(unravel(ps), val_images[:128]).argmax(axis=1))(particles.rundata['particles'][-1])\n",
    "onp.unique(all_preds, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue with scheduled programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
