{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a BNN to classify MNIST using neural SVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--xla_gpu_cuda_data_dir=/cluster/apps/gcc-6.3.0/cuda-10.1.243-n6qg6z5js3zfnhp2cfg5yjccej636czm'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for leonhard\n",
    "import os\n",
    "os.environ['XLA_FLAGS'] = \"--xla_gpu_cuda_data_dir=\" + os.environ[\"CUDA_HOME\"]\n",
    "os.environ['XLA_FLAGS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Bayesian neural network to classify MNIST using\n",
    "# Neural SVGD\n",
    "#\n",
    "# If using pmap, set the environment variable\n",
    "# `export XLA_FLAGS=\"--xla_force_host_platform_device_count=8\"`\n",
    "# before running on CPU (this enables pmap to \"see\" multiple cores).\n",
    "%load_ext autoreload\n",
    "import sys\n",
    "import os\n",
    "on_cluster = not os.getenv(\"HOME\") == \"/home/lauro\"\n",
    "if on_cluster:\n",
    "    sys.path.append(\"/cluster/home/dlauro/projects-2020-Neural-SVGD/learning_particle_gradients/\")\n",
    "sys.path.append(\"../../experiments/\")\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from itertools import cycle\n",
    "\n",
    "import numpy as onp\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import jit, grad, value_and_grad, vmap, pmap, config, random\n",
    "config.update(\"jax_debug_nans\", False)\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "import nets\n",
    "import utils\n",
    "import models\n",
    "import metrics\n",
    "from convnet import make_model, accuracy, crossentropy_loss, log_prior, ensemble_accuracy\n",
    "\n",
    "import os\n",
    "model = make_model(\"small\")\n",
    "\n",
    "# Config\n",
    "key = random.PRNGKey(0)\n",
    "EPOCHS = 1\n",
    "# NUM_VALS = 20\n",
    "\n",
    "META_LEARNING_RATE = 1e-3\n",
    "\n",
    "NUM_SAMPLES = 150\n",
    "DISABLE_PROGRESS_BAR = False\n",
    "USE_PMAP = False\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LAMBDA_REG = 10**2\n",
    "STEP_SIZE = 1e-4 #1e-3, 1e-4 too large (w adam)\n",
    "PATIENCE = 5\n",
    "MAX_TRAIN_STEPS = 100 #15\n",
    "\n",
    "\n",
    "if USE_PMAP:\n",
    "    vpmap = pmap\n",
    "else:\n",
    "    vpmap = vmap\n",
    "\n",
    "# Load MNIST\n",
    "data_dir = './data' if on_cluster else '/tmp/tfds'\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
    "mnist_data = tfds.as_numpy(mnist_data)\n",
    "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
    "\n",
    "# Full train and test set\n",
    "train_images, train_labels = train_data['image'], train_data['label']\n",
    "test_images, test_labels = test_data['image'], test_data['label']\n",
    "\n",
    "# Split off the validation set\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.1, random_state=0)\n",
    "data_size = len(train_images)\n",
    "\n",
    "\n",
    "def make_batches(images, labels, batch_size):\n",
    "    \"\"\"Returns an iterator that cycles through \n",
    "    tuples (image_batch, label_batch).\"\"\"\n",
    "    num_batches = len(images) // batch_size\n",
    "    split_idx = onp.arange(1, num_batches+1)*batch_size\n",
    "    batches = zip(*[onp.split(data, split_idx, axis=0) for data in (images, labels)])\n",
    "    return cycle(batches)\n",
    "\n",
    "\n",
    "def loss(params, images, labels):\n",
    "    \"\"\"Minibatch approximation of the (unnormalized) Bayesian\n",
    "    negative log-posterior evaluated at `params`. That is,\n",
    "    -log model_likelihood(data_batch | params) * batch_rescaling_constant - log prior(params))\"\"\"\n",
    "    logits = model.apply(params, images)\n",
    "    return data_size/BATCH_SIZE * crossentropy_loss(logits, labels) - log_prior(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural SVGD Model\n",
    "* Input: model parameters\n",
    "* Output: 'gradient' of same shape as parameters\n",
    "\n",
    "### Memory\n",
    "Assume: convnet model has 50.000 parameters, and NSVGD layers have hidden dimensions `[1024, 1024]`. Then the NSVGD (meta) model has `2*50.000*1024` parameters (each 8 bytes), i.e. about 800MB.\n",
    "\n",
    "The activations: `50.000 + 1024 + 1024 + 50.000` floats, i.e. about 800KB, times `NUM_SAMPLES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--xla_gpu_cuda_data_dir=/cluster/apps/gcc-6.3.0/cuda-10.1.243-n6qg6z5js3zfnhp2cfg5yjccej636czm\r\n"
     ]
    }
   ],
   "source": [
    "!echo $XLA_FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for dealing with parameters\n",
    "key, subkey = random.split(key)\n",
    "params_tree = model.init(subkey, train_images[:2])\n",
    "params_flat, unravel = jax.flatten_util.ravel_pytree(params_tree)\n",
    "\n",
    "\n",
    "def ravel(tree):\n",
    "    return jax.flatten_util.ravel_pytree(tree)[0]\n",
    "\n",
    "\n",
    "def init_flat_params(key):\n",
    "    return ravel(model.init(key, train_images[:2]))\n",
    "\n",
    "\n",
    "def get_minibatch_logp(batch):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        batch = (images, labels)\n",
    "\n",
    "    Returns a callable that computes target posterior\n",
    "    given flattened param vector.\n",
    "    \"\"\"\n",
    "    def minibatch_logp(params_flat):\n",
    "        return -loss(unravel(params_flat), *batch)\n",
    "    return minibatch_logp\n",
    "\n",
    "\n",
    "def sample_tv(key):\n",
    "    \"\"\"return two sets of particles at initialization, for\n",
    "    training and validation in the warmup phase\"\"\"\n",
    "    return vmap(init_flat_params)(random.split(key, NUM_SAMPLES)).split(2)\n",
    "\n",
    "\n",
    "@jit\n",
    "def acc(param_set_flat):\n",
    "    param_set = vmap(unravel)(param_set_flat)\n",
    "    logits = vmap(model.apply, (0, None))(param_set, val_images[:BATCH_SIZE])\n",
    "    return ensemble_accuracy(logits, val_labels[:BATCH_SIZE])\n",
    "\n",
    "\n",
    "def vmean(fun):\n",
    "    \"\"\"vmap, but computes mean along mapped axis\"\"\"\n",
    "    def compute_mean(*args, **kwargs):\n",
    "        return jnp.mean(vmap(fun)(*args, **kwargs), axis=-1)\n",
    "    return compute_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = make_batches(train_images, train_labels, BATCH_SIZE)\n",
    "test_batches  = make_batches(test_images,  test_labels, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init particles and dynamics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "init_particles = vmap(init_flat_params)(random.split(subkey, NUM_SAMPLES))\n",
    "\n",
    "opt = optax.sgd(STEP_SIZE)\n",
    "\n",
    "# opt = optax.chain(\n",
    "#     optax.scale_by_adam(),\n",
    "#     optax.scale(-STEP_SIZE),\n",
    "# )\n",
    "\n",
    "key, subkey1, subkey2 = random.split(key, 3)\n",
    "neural_grad = models.SDLearner(target_dim=init_particles.shape[1],\n",
    "                               get_target_logp=get_minibatch_logp,\n",
    "                               learning_rate=META_LEARNING_RATE,\n",
    "                               key=subkey1,\n",
    "                               sizes=[256, 256, 128, init_particles.shape[1]],\n",
    "                               aux=False,\n",
    "                               use_hutchinson=True,\n",
    "                               lambda_reg=LAMBDA_REG,\n",
    "                               patience=PATIENCE,\n",
    "                               dropout=True)\n",
    "particles = models.Particles(subkey2, neural_grad.gradient, init_particles, custom_optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximal initial stein discrepancy (theoretical ideal)\n",
    "import plot\n",
    "import stein\n",
    "from jax.scipy import stats\n",
    "\n",
    "n, d = init_particles.shape\n",
    "n = 100\n",
    "key, subkey = random.split(key)\n",
    "xs = random.normal(subkey, (n, d)) / 100\n",
    "\n",
    "first_batch = next(train_batches)\n",
    "images, labels = first_batch\n",
    "\n",
    "def logp(x):\n",
    "    def loglikelihood(x):\n",
    "        params = unravel(x)\n",
    "        logits = model.apply(params, images)\n",
    "        return -data_size/BATCH_SIZE * crossentropy_loss(logits, labels)\n",
    "    return loglikelihood(x) + log_prior(unravel(x))\n",
    "\n",
    "\n",
    "def logq(x):\n",
    "    return stats.norm.logpdf(x, loc=0, scale=1/100).sum()\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    def loglikelihood(x):\n",
    "        params = unravel(x)\n",
    "        logits = model.apply(params, images)\n",
    "        return -data_size/BATCH_SIZE * crossentropy_loss(logits, labels) / (2*LAMBDA_REG)\n",
    "    return grad(loglikelihood)(x)\n",
    "\n",
    "# double-check stein discrepancy\n",
    "\n",
    "# a) true\n",
    "l2 = utils.l2_norm_squared(xs, f) # = sd(f*) / (2 LAMBDA_REG)\n",
    "true_sd = 2 * LAMBDA_REG * l2\n",
    "min_loss = -l2 * LAMBDA_REG\n",
    "\n",
    "# b) hutchinson\n",
    "key, subkey = random.split(key)\n",
    "random_estimate_sd = stein.stein_discrepancy_hutchinson(subkey, xs, logp, f)\n",
    "\n",
    "print(\"analytical sd:\", true_sd)\n",
    "print(\"hutchinson estimate:\", random_estimate_sd)\n",
    "print(\"ratio:\", true_sd / random_estimate_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup on first batch\n",
    "NUM_WARMUP_ITER = 5\n",
    "key, subkey = random.split(key)\n",
    "neural_grad.warmup(key=subkey,\n",
    "                   sample_split_particles=sample_tv,\n",
    "                   next_data=lambda: first_batch,\n",
    "                   n_iter=NUM_WARMUP_ITER,\n",
    "                   n_inner_steps=100,\n",
    "                   progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=[15, 5])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(neural_grad.rundata['training_loss'])\n",
    "ax.plot(neural_grad.rundata['validation_loss'])\n",
    "ax.axhline(y=min_loss, label=\"true first-batch loss\", linestyle=\"--\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(neural_grad.rundata['training_sd'])\n",
    "ax.plot(neural_grad.rundata['validation_sd'])\n",
    "ax.axhline(y=true_sd, linestyle=\"--\", label=\"true first-batch stein discrepancy\")\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(neural_grad.rundata['l2_norm'])\n",
    "ax.axhline(y=l2, linestyle=\"--\", label=\"true l2 norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_grad.rundata['train_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PARTICLE_STEPS = 10\n",
    "\n",
    "key, fixed_key = random.split(key)\n",
    "def step(key, train_batch):\n",
    "    \"\"\"one iteration of the particle trajectory simulation\"\"\"\n",
    "    neural_grad.train(\n",
    "        split_particles=particles.next_batch(key),\n",
    "        n_steps=MAX_TRAIN_STEPS,\n",
    "        data=train_batch\n",
    "    )\n",
    "    for _ in range(N_PARTICLE_STEPS):\n",
    "        particles.step(neural_grad.get_params())\n",
    "\n",
    "\n",
    "@jit\n",
    "def compute_eval(train_batch, ps):\n",
    "    test_logp = vmean(get_minibatch_logp(next(test_batches)))\n",
    "    train_logp = vmean(get_minibatch_logp(train_batch))\n",
    "    stepdata = {\n",
    "        \"accuracy\": acc(ps),\n",
    "        \"test_logp\": test_logp(ps),\n",
    "        \"training_logp\": train_logp(ps),\n",
    "        \"particle_mean\": ps.mean(),\n",
    "    }\n",
    "    return stepdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* currently testing what happens if we **fix** train/val splits\n",
    "    * outcome: no overfit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_steps = EPOCHS * data_size // BATCH_SIZE // 5\n",
    "num_steps = 51\n",
    "for step_counter in tqdm(range(num_steps)):\n",
    "    key, subkey = random.split(key)\n",
    "    train_batch = next(train_batches)\n",
    "    step(subkey, train_batch)\n",
    "#     if step_counter % (num_steps//NUM_VALS) == 0:\n",
    "    if step_counter % 10 == 0:\n",
    "        metrics.append_to_log(particles.rundata,\n",
    "                              compute_eval(train_batch, particles.particles))\n",
    "# neural_grad.done()\n",
    "# particles.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=[15, 5])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(particles.rundata[\"accuracy\"], \"--.\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(neural_grad.rundata['training_loss'], \".\", label=\"training loss\")\n",
    "ax.plot(neural_grad.rundata['validation_loss'], \".\", label=\"validation loss\")\n",
    "ax.axhline(y=min_loss, label=\"loss should never go below here\", linestyle=\"--\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(particles.rundata['training_logp'])\n",
    "# ax.plot(particles.rundata['test_logp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onp.mean(particles.rundata[\"accuracy\"][-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=[15, 5])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(particles.rundata[\"accuracy\"], \"--.\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(neural_grad.rundata['training_loss'], \".\", label=\"training loss\")\n",
    "ax.plot(neural_grad.rundata['validation_loss'], \".\", label=\"validation loss\")\n",
    "ax.axhline(y=min_loss, label=\"loss should never go below here\", linestyle=\"--\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(particles.rundata['training_logp'])\n",
    "# ax.plot(particles.rundata['test_logp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neural_grad.rundata[\"global_gradient_norm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles.rundata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = onp.array(particles.rundata['particles'])\n",
    "trajectories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize trajectory avg across dimensions (distinguish particles)\n",
    "fig, axs = plt.subplots(2, 1, figsize=[10, 8])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(trajectories.mean(axis=2));  # avg across dims\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(trajectories[:, :, 1]);  # watch single param (aka single dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize trajectory avg across particles (distinguish dims, ie parameters)\n",
    "fig, axs = plt.subplots(2, 1, figsize=[10, 8])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(trajectories.mean(axis=1)); # avg across particles\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(trajectories[:, 11, :]); # watch single particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate markings based on num_steps\n",
    "plt.subplots(figsize=[15, 5])\n",
    "markings = onp.cumsum(neural_grad.rundata['train_steps'])[NUM_WARMUP_ITER:]\n",
    "\n",
    "plt.plot(neural_grad.rundata['training_loss'])\n",
    "plt.plot(neural_grad.rundata['validation_loss'])\n",
    "plt.scatter(markings, onp.zeros(len(markings)))\n",
    "plt.axhline(y=min_loss, label=\"loss should never go below here\", linestyle=\"--\", color=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 800\n",
    "b = 1000\n",
    "marker_a = (markings < a).sum()\n",
    "marker_b = (markings < b).sum()\n",
    "print(f\"step {marker_a} to step {marker_b}.\")\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=[10, 10])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(neural_grad.rundata['training_loss'][a:b], \".\")\n",
    "ax.plot(neural_grad.rundata['validation_loss'][a:b], \".\")\n",
    "ax.scatter(markings[marker_a:marker_b] - a, onp.zeros(marker_b - marker_a))\n",
    "ax.axhline(y=min_loss, label=\"loss should never go below here\", linestyle=\"--\", color=\"g\")\n",
    "ax.legend()\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(neural_grad.rundata['l2_norm'][a:b])\n",
    "ax.scatter(markings[marker_a:marker_b] - a, onp.zeros(marker_b - marker_a))\n",
    "# ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# particle gradient norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = neural_grad.grads(particles.particles)\n",
    "optax.global_norm(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"norm pre upate :\", particles.rundata['global_grad_norm'][0])\n",
    "print(\"norm post upate:\", particles.rundata['global_grad_norm_post_update'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(particles.rundata['global_grad_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(particles.rundata['global_grad_norm_post_update'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optax.global_norm(vmap(unravel)(particles.particles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.sqrt(jnp.sum(particles.particles**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model gradient norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interlude: why accuracy no change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = vmap(lambda ps: model.apply(unravel(ps), val_images[:128]).argmax(axis=1))(particles.rundata['particles'][-1])\n",
    "onp.unique(all_preds, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue with scheduled programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
