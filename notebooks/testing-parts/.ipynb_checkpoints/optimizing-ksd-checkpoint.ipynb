{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the KSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/lib/xla_bridge.py:125: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/lauro/code/msc-thesis/svgd/kernel_learning/\")\n",
    "import json_tricks as json\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, random, lax, jacfwd, value_and_grad\n",
    "from jax import lax\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as onp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import haiku as hk\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import config\n",
    "\n",
    "import utils\n",
    "import metrics\n",
    "import time\n",
    "import plot\n",
    "import stein\n",
    "import kernels\n",
    "import svgd\n",
    "\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rbf_fn(bandwidth):\n",
    "    def rbf(x, y): \n",
    "        x, y = np.asarray(x), np.asarray(y)\n",
    "        return np.exp(- np.sum((x - y)**2 / bandwidth**2) / 2)\n",
    "    return rbf\n",
    "\n",
    "# other kernels for comparison\n",
    "def get_tophat_fn(bandwidth):\n",
    "    def tophat(x, y): return np.squeeze(np.where(np.linalg.norm(x-y)<bandwidth, 1., 0.))\n",
    "    return tophat\n",
    "\n",
    "def constant(x, y): return np.array(1.)\n",
    "\n",
    "def funnelize(v):\n",
    "    \"\"\"If v is standard 2D normal, then\n",
    "    funnelize(v) is distributed as Neal's Funnel.\"\"\"\n",
    "    x, y = v\n",
    "    return np.append(x*np.exp(3*y/2), 3*y)\n",
    "\n",
    "def defunnelize(z):\n",
    "    \"\"\"Inverse of funnelize.\"\"\"\n",
    "    x, y = z\n",
    "    return np.append(x*np.exp(-y/2), y/3)\n",
    "\n",
    "def get_funnel_fn(bandwidth):\n",
    "    rbf = get_rbf_fn(bandwidth)\n",
    "    def funnel_kernel(x, y):\n",
    "        return rbf(defunnelize(x), defunnelize(y))\n",
    "    return funnel_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSDLearner():\n",
    "    def __init__(self, key, sizes, activation_kernel, proposal, target, learning_rate: float):\n",
    "        self.sizes = sizes\n",
    "        self.activation_kernel = activation_kernel\n",
    "        self.proposal = proposal\n",
    "        self.target = target\n",
    "        \n",
    "        # net and optimizer\n",
    "        self.mlp = kernels.build_mlp(self.sizes, name=\"MLP\", skip_connection=False)\n",
    "        self.opt = svgd.Optimizer(*optimizers.adam(learning_rate))\n",
    "        self.step = 0\n",
    "        self.losses = []\n",
    "        self.initialize_optimizer(key)\n",
    "            \n",
    "    def initialize_optimizer(self, key):\n",
    "        # initialize optimizer\n",
    "        x_dummy = np.ones(self.proposal.d)\n",
    "        key, subkey = random.split(key)\n",
    "        init_params = self.mlp.init(subkey, x_dummy)\n",
    "        self.optimizer_state = self.opt.init(init_params)\n",
    "        return None\n",
    "    \n",
    "    def get_kernel_fn(self, params):\n",
    "        def kernel(x, y):\n",
    "            x, y = np.asarray(x), np.asarray(y)\n",
    "            return self.activation_kernel(self.mlp.apply(params, None, x), \n",
    "                                          self.mlp.apply(params, None, y))\n",
    "        return kernel\n",
    "\n",
    "    def loss_fn(self, params, samples):\n",
    "        kernel = self.get_kernel_fn(params)\n",
    "        ksd = stein.ksd_squared_u(samples, self.target.logpdf, kernel, False)\n",
    "        return -ksd\n",
    "\n",
    "    @partial(jit, static_argnums=0)\n",
    "    def update_step(self, optimizer_state, samples, step: int):\n",
    "        # update step\n",
    "        params = self.opt.get_params(optimizer_state)\n",
    "        loss, g = value_and_grad(self.loss_fn)(params, samples)\n",
    "        optimizer_state = self.opt.update(step, g, optimizer_state)\n",
    "        return optimizer_state, loss\n",
    "        \n",
    "    def train(self, n_steps=100, n_samples=400):\n",
    "        for _ in tqdm(range(n_steps)):\n",
    "            self.step += 1\n",
    "            samples = proposal.sample(n_samples)\n",
    "            self.optimizer_state, loss = self.update_step(self.optimizer_state, samples, self.step)\n",
    "            self.losses.append(loss)\n",
    "        return None\n",
    "    \n",
    "    @partial(jit, static_argnums=0)    \n",
    "    def estimate_ksd_squared(self, params):\n",
    "        k = self.get_kernel_fn(params)\n",
    "        ksd = stein.ksd_squared_u(samples, self.target.logpdf, k, False)\n",
    "        \n",
    "    def compute_final_loss_mean_and_stddev(self):\n",
    "        loss_tail = self.losses[-50:]\n",
    "        return onp.mean(loss_tail), onp.std(loss_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate KSD and Var(\\hat KSD) values for base kernels\n",
    "def estimate_ksd_and_stddev(n: int, m: int, kernels, proposal, target):\n",
    "    \"\"\"\n",
    "    n: number of particles\n",
    "    m: number of draws for variance estimation\n",
    "    kernels: list of kernel functions\n",
    "    \n",
    "    Returns\n",
    "    ksds: mean ksd value for each kernel\n",
    "    stds: standard dev for each kernel\n",
    "    \"\"\"\n",
    "    ksds_list = [[] for kernel in kernels]\n",
    "    for _ in tqdm(range(m)):\n",
    "        samples = proposal.sample(n)\n",
    "        for kernel_ksds, kernel in zip(ksds_list, kernels):\n",
    "            ksd = stein.ksd_squared_u(samples, target.logpdf, kernel, False)\n",
    "            kernel_ksds.append(ksd)\n",
    "    \n",
    "    ksds = onp.mean(ksds_list, axis=1)\n",
    "    stds = onp.std(ksds_list, axis=1)\n",
    "    return ksds, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(losses, ksds, stds, ax=None, lims=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    grid_ksds = -np.asarray(learner.losses)\n",
    "    ax.plot(grid_ksds, \"--k\", label=\"Optimized KSD\")\n",
    "\n",
    "    labels = (\"k(x, y)=1 constant\", \"RBF with bandwidth 1\", \"RBF composed with funnel\")\n",
    "    cycle = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "    for label, ksd, std, errorbar_xpos in zip(labels, ksds, stds, len(grid_ksds)*np.array([1/3, 2/3, 0.9])):\n",
    "        col = next(cycle)\n",
    "        ax.axhline(y=ksd, label=label, color=col, linestyle=\"--\")\n",
    "        ax.errorbar(errorbar_xpos, ksd, yerr=std, color=col, capsize=5, label=\"Standard deviation\")\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"KSD\")\n",
    "\n",
    "    ax.set_ylim(lims)\n",
    "    _ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal = metrics.FunnelizedGaussian([1, 1], 1)\n",
    "target = metrics.Funnel(2)\n",
    "sizes = [32, 32, 32, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training kernel to optimize KSD...\")\n",
    "learner = KSDLearner(key, sizes, get_rbf_fn(1), proposal, target, 0.01)\n",
    "learner.train(200)\n",
    "loss_mean, loss_std = learner.compute_final_loss_mean_and_stddev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing KSDs with base kernels for comparison...\")\n",
    "kernel_list = [constant, get_rbf_fn(1)]\n",
    "if isinstance(target, metrics.Funnel):\n",
    "    kernel_list.append(get_funnel_fn(1))\n",
    "ksds, stds = onp.array(estimate_ksd_and_stddev(400, 15, kernel_list, \n",
    "                                              proposal, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plot results:\")\n",
    "fig, ax = plt.subplots(figsize=[10,8])\n",
    "plot_training(learner.losses, ksds, stds, ax)\n",
    "_ = ax.errorbar(len(learner.losses)-25, -loss_mean, yerr=loss_std, capsize=5, color=\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_kernel = learner.get_kernel_fn(learner.opt.get_params(learner.optimizer_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = proposal.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_kernel(s[0], s[40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check\n",
    "In theory, if $k$ is the constant kernel $k(x, y) = 1$, then \n",
    "KSD rel to $k$ should be equal to \n",
    "$\\Vert \\phi^* \\Vert$\n",
    "where $\\phi^* = E_{x \\sim q}[\\nabla \\log p(x)] \\in \\mathbb R^d$.\n",
    "\n",
    "So KSD squared is equal to $\\Vert \\phi^* \\Vert^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogp = grad(target.logpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_theory(x, y):\n",
    "    return np.inner(dlogp(x), dlogp(y))\n",
    "\n",
    "def h(x, y):\n",
    "    def inner(x):\n",
    "        kx = lambda y_: np.array(1.)\n",
    "        return stein.stein_operator(kx, y, target.logpdf)\n",
    "    return stein.stein_operator(inner, x, target.logpdf, transposed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = proposal.sample(2)\n",
    "h_theory(x, y) == h(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 400\n",
    "m = 15\n",
    "estimate_ksd_and_stddev(n, m, (constant,), proposal, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = proposal.sample(100000)\n",
    "exp_grad_log = np.mean(vmap(dlogp)(samples), axis=0)\n",
    "print(np.linalg.norm(exp_grad_log)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
