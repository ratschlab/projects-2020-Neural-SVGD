{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute $\\text{KL}(q_{T_\\varepsilon} \\Vert p)$, the KL div after an SVGD step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/lib/xla_bridge.py:125: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "from jax import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/lauro/code/msc-thesis/svgd/kernel_learning/\")\n",
    "import json_tricks as json\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, random, lax, jacfwd, value_and_grad\n",
    "from jax import lax\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as onp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import haiku as hk\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import config\n",
    "\n",
    "import utils\n",
    "import metrics\n",
    "import time\n",
    "import plot\n",
    "import stein\n",
    "import kernels\n",
    "import svgd\n",
    "import distributions\n",
    "import nets\n",
    "\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, \n",
    "$$\\text{KSD}(q \\Vert p)^2 \\approx \\dfrac{d}{d \\varepsilon} \\text{KL}(q_{T_\\varepsilon} \\Vert p) \\Big \\vert_{\\varepsilon=0}$$\n",
    "when $T_\\varepsilon(x) = x + \\varepsilon \\phi^*(x)$.\n",
    "\n",
    "We can estimate the RHS term by \n",
    "* taking one SVGD step $x'_i = x_i + \\hat \\phi^*(x_i)$, and estimate directly\n",
    "$$\\text{KL}(q_{x} \\Vert p(x)) - \\text{KL}(q_{x'} \\Vert p)$$\n",
    "    - could compute $q$ by inverting the Jacobian of $T$.t\n",
    "* estimating $\\text{KL}(q \\Vert p)$ and differentiating estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that\n",
    "$$\n",
    "\\text{KL}(q_T \\Vert p) = \\text{KL}(q \\Vert p_{T^{-1}}) = E_{x \\sim q} [\\log q(x) - \\log p(T x) - \\log \\det J_T(x)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_kl(logq, logp, samples):\n",
    "    return np.mean(vmap(logq)(samples) - vmap(logp)(samples))\n",
    "\n",
    "def pushforward_log(logpdf: callable, tinv: callable):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "        logpdf computes log(p(_)), where p is a PDF.\n",
    "        tinv is the inverse of an injective transformation T: R^d --> R^d, x --> z\n",
    "    \n",
    "    Returns\n",
    "        $\\log p_T(z)$, where z = T(x). That is, the pushforward log pdf \n",
    "        $$\\log p_T(z) = \\log p(T^{-1} z) + \\log \\det(J_{T^{-1} z})$$\n",
    "    \"\"\"\n",
    "    def pushforward_logpdf(z):\n",
    "        det = np.linalg.det(jacfwd(tinv)(z))\n",
    "#         if np.abs(det) < 0.001:\n",
    "#             raise LinalgError(\"Determinant too small: T is not injective.\")\n",
    "        return logpdf(tinv(z)) + np.log(np.abs(det))\n",
    "    return pushforward_logpdf\n",
    "\n",
    "def kl_diff(logq, logp, x, transform):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        logq: computes log(q(x))\n",
    "        logp: computes log(p(x))\n",
    "        x: n samples from q, shape (n, d)\n",
    "        transform: function T: R^d --> R^d, x --> z\n",
    "    \"\"\"\n",
    "    # KL(q || p)\n",
    "    kl1 = estimate_kl(logq, logp, x)\n",
    "    \n",
    "    # KL(q_T || p) = KL(q || p_{T^{-1}})\n",
    "    z = vmap(transform)(x)\n",
    "    logp_pullback = pushforward_log(logp, transform)\n",
    "    kl2 = estimate_kl(logq, logp_pullback, x)\n",
    "    return kl1 - kl2\n",
    "\n",
    "# @partial(jit, static_argnums=(2, 3, 4, 5))\n",
    "def get_kl_diff_and_ksd(key, eps, proposal, target, kernel, n_samples):\n",
    "    proposal.threadkey = key\n",
    "    x = proposal.sample(n_samples)\n",
    "    print(x.shape)\n",
    "    def transform(x):\n",
    "        return x + eps * stein.phistar_i(x, samples, target.logpdf, kernel, aux=False)\n",
    "    return kl_diff(proposal.logpdf, target.logpdf, x, transform), stein.ksd_squared_u(x, target.logpdf, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "proposal = distributions.Gaussian(0, 1)\n",
    "target   = distributions.Gaussian(4, 1)\n",
    "eps = 0.001\n",
    "trafo = lambda x: x + eps\n",
    "dkl = kl_diff(proposal.logpdf, target.logpdf, proposal.sample(800), trafo) / eps # estimate of - d/deps KL(q_T_eps || p) \n",
    "\n",
    "def steinop(x):\n",
    "    return stein.stein_operator(lambda x: np.asarray(1.), x, target.logpdf, transposed=False)\n",
    "xs = proposal.sample(800)\n",
    "stein_dkl = np.mean(vmap(steinop)(xs))\n",
    "\n",
    "print(dkl - stein_dkl < 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KSD test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal = distributions.Gaussian(0, 1)\n",
    "target   = distributions.Gaussian(0, 9)\n",
    "eps = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSD = 0.14718269\n",
      "d/deps KL = 0.16075373\n",
      "absolute difference: 0.013571039\n",
      "relative difference: 0.08814182\n"
     ]
    }
   ],
   "source": [
    "# rbf kernel\n",
    "kernel = kernels.get_rbf_kernel(bandwidth=1)\n",
    "ksd = stein.ksd_squared_u(proposal.sample(400), target.logpdf, kernel)\n",
    "qs = proposal.sample(400)\n",
    "def trafo(x):\n",
    "    return x + eps * stein.phistar_i(x, qs, target.logpdf, kernel, aux=False)\n",
    "dkl = kl_diff(proposal.logpdf, target.logpdf, proposal.sample(800), trafo) / eps\n",
    "print(\"KSD =\", ksd)\n",
    "print(\"d/deps KL =\", dkl)\n",
    "print(\"absolute difference:\", dkl - ksd)\n",
    "print(\"relative difference:\", (dkl - ksd) / ((dkl + ksd) / 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSD = -6.80155e-06\n",
      "d/deps KL = 5.9604645e-06\n",
      "absolute difference: 1.2762015e-05\n",
      "relative difference: -30.346527\n"
     ]
    }
   ],
   "source": [
    "# constant kernel\n",
    "kernel = kernels.constant_kernel\n",
    "ksd = stein.ksd_squared_u(proposal.sample(800), target.logpdf, kernel)\n",
    "qs = proposal.sample(400)\n",
    "def trafo(x):\n",
    "    return x + eps * stein.phistar_i(x, qs, target.logpdf, kernel, aux=False)\n",
    "dkl = kl_diff(proposal.logpdf, target.logpdf, proposal.sample(800), trafo) / eps\n",
    "print(\"KSD =\", ksd)\n",
    "print(\"d/deps KL =\", dkl)\n",
    "print(\"absolute difference:\", dkl - ksd)\n",
    "print(\"relative difference:\", (dkl - ksd) / ((dkl + ksd) / 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
