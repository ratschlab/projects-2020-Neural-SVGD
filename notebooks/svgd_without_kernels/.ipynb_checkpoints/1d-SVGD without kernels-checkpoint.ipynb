{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVGD without kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "from jax import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "# config.update(\"jax_disable_jit\", True)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/lauro/code/msc-thesis/svgd/kernel_learning/\")\n",
    "import json_tricks as json\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, random, lax, jacfwd, value_and_grad\n",
    "from jax import lax\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as onp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import haiku as hk\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import config\n",
    "\n",
    "import utils\n",
    "import metrics\n",
    "import time\n",
    "import plot\n",
    "import stein\n",
    "import kernels\n",
    "import distributions\n",
    "import nets\n",
    "import models\n",
    "\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "key = random.PRNGKey(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup = distributions.double_mixture\n",
    "# target, proposal = setup.get()\n",
    "target = distributions.GaussianMixture([-3, 3], [1, 1], [1/3, 2/3])\n",
    "proposal = distributions.Gaussian(-5, 1)\n",
    "setup = distributions.Setup(target, proposal)\n",
    "sizes = [32, 32, 1]\n",
    "learning_rate = 1e-2\n",
    "particle_lr = 1e-1\n",
    "lambda_reg = 1\n",
    "n_particles = 1000 # (num samples)\n",
    "sample_every = False\n",
    "noise_level = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rescale $\\phi^*$ by\n",
    "$$\n",
    "\\alpha = \\frac{\\text{SD}(\\phi^*)}{2 \\lambda \\Vert \\phi^* \\Vert_{L^2(q)}^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phistar_batched(params, _, particles, aux=False):\n",
    "    kernel = kernels.get_rbf_kernel(1)\n",
    "    inducing_particles = params\n",
    "    phi = stein.get_phistar(kernel, target.logpdf, inducing_particles)\n",
    "    l2_phi_squared = utils.l2_norm(inducing_particles, phi)**2\n",
    "    ksd = stein.stein_discrepancy(inducing_particles, target.logpdf, phi)\n",
    "    alpha = ksd / (2*lambda_reg*l2_phi_squared)\n",
    "    if aux:\n",
    "        return -alpha*vmap(phi)(particles), {\"ksd\": ksd, \"alpha\": alpha}\n",
    "    else:\n",
    "        return -alpha*vmap(phi)(particles)\n",
    "\n",
    "\n",
    "def phistar_batched_adaptive(params, _, particles, aux=False):\n",
    "    inducing_particles = params\n",
    "    bandwidth = kernels.median_heuristic(inducing_particles)\n",
    "    kernel = kernels.get_rbf_kernel(bandwidth)\n",
    "    phi = stein.get_phistar(kernel, target.logpdf, inducing_particles)\n",
    "    l2_phi_squared = utils.l2_norm(inducing_particles, phi)**2\n",
    "    ksd = stein.stein_discrepancy(inducing_particles, target.logpdf, phi)\n",
    "    alpha = ksd / (2*lambda_reg*l2_phi_squared)\n",
    "    if aux:\n",
    "        return -alpha*vmap(phi)(particles), {\"ksd\": ksd, \"alpha\": alpha}\n",
    "    else:\n",
    "        return -alpha*vmap(phi)(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "stein_learner = models.SDLearner(subkey,\n",
    "                                 target=target,\n",
    "                                 sizes=sizes,\n",
    "                                 learning_rate=learning_rate,\n",
    "                                 lambda_reg=lambda_reg)\n",
    "key, subkey = random.split(key)\n",
    "particles_score = models.Particles(subkey,\n",
    "                             gradient=stein_learner.kl_gradient,\n",
    "                             proposal=proposal,\n",
    "                             n_particles=n_particles,\n",
    "                             learning_rate=particle_lr,\n",
    "                             only_training=True)\n",
    "\n",
    "particles_svgd = models.Particles(subkey,\n",
    "                             gradient=phistar_batched,\n",
    "                             proposal=proposal,\n",
    "                             n_particles=n_particles,\n",
    "                             learning_rate=particle_lr,\n",
    "                             only_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVGD and SVGD without kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_schedule(step_count):\n",
    "    \"\"\"Return nr of stein learner iterations\n",
    "    at (particle) step step_count\"\"\"\n",
    "    if step_count < 2:\n",
    "        return 150\n",
    "    else:\n",
    "        return 50 if step_count < 10 else 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "n_steps=1000\n",
    "n_float_errs = 0\n",
    "for i in tqdm(range(n_steps)):\n",
    "    n_learner_steps = step_schedule(i)\n",
    "    train_x, val_x = particles_score.get_params(split_by_group=True)\n",
    "    try:\n",
    "        x = stein_learner.train(train_x, val_x, key=subkey, n_steps=n_learner_steps, noise_level=noise_level)\n",
    "    except FloatingPointError:\n",
    "        n_float_errs += 1\n",
    "        particles_score.perturb()\n",
    "        ...\n",
    "    particles_score.step(stein_learner.get_params())\n",
    "\n",
    "    inducing_particles, _ = particles_svgd.get_params(split_by_group=True)\n",
    "    particles_svgd.step(inducing_particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim1 = (-10, 1.5)\n",
    "ylim2 = (1, 3.5)\n",
    "# ylim1 = (-0.5, 1.5)\n",
    "# ylim2 = (4.5, 5.6)\n",
    "# ylim1=None\n",
    "# ylim2=None\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=[18,5])\n",
    "axs = axs.flatten()\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(particles_score.rundata[\"training_mean\"], label=\"SVGD-learned mean\")\n",
    "ax.axhline(y=target.mean, linestyle=\"--\", color=\"green\")\n",
    "ax.set_ylim(ylim1)\n",
    "ax.plot(particles_svgd.rundata[\"training_mean\"], label=\"SVGD mean\")\n",
    "ax.axhline(y=target.mean, linestyle=\"--\", color=\"green\")\n",
    "ax.set_ylim(ylim1)\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(particles_score.rundata[\"training_std\"], label=\"SVGD-learned std\")\n",
    "# ax.plot(particles_score.rundata[\"validation_std\"])\n",
    "ax.axhline(y=np.sqrt(target.cov), linestyle=\"--\", color=\"green\")\n",
    "ax.set_ylim(ylim2)\n",
    "ax.plot(particles_svgd.rundata[\"training_std\"], label=\"SVGD std\")\n",
    "ax.axhline(y=np.sqrt(target.cov), linestyle=\"--\", color=\"green\")\n",
    "ax.set_ylim(ylim2)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=[16, 6])\n",
    "axs = axs.flatten()\n",
    "x_train_sl, single_val = particles_score.get_params(split_by_group=True)\n",
    "x_train_svgd, _ = particles_svgd.get_params(split_by_group=True)\n",
    "\n",
    "for ax, x_train in zip(axs, (x_train_sl, x_train_svgd)):\n",
    "    ax.hist(x_train[:,0], density=True, alpha=0.5, bins=25)\n",
    "    plot.plot_fun(target.pdf, ax=ax, lims=(-15, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stein_learner.rundata[\"fnorm\"])\n",
    "plt.ylim(-0.1, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
