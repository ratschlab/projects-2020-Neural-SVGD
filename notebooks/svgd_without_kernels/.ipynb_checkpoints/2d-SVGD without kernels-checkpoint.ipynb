{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVGD without kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "from jax import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "# config.update(\"jax_disable_jit\", True)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/lauro/code/msc-thesis/svgd/kernel_learning/\")\n",
    "import json_tricks as json\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, random, lax, jacfwd, value_and_grad\n",
    "from jax import lax\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as onp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import haiku as hk\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import config\n",
    "\n",
    "import utils\n",
    "import metrics\n",
    "import time\n",
    "import plot\n",
    "import stein\n",
    "import kernels\n",
    "import distributions\n",
    "import nets\n",
    "import models\n",
    "\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "key = random.PRNGKey(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = distributions.Banana([0,0], [4,1])\n",
    "proposal = distributions.Gaussian([0,0], [9,9])\n",
    "setup = distributions.Setup(target, proposal)\n",
    "\n",
    "sizes = [32, 32, 2]\n",
    "learning_rate = 1e-2\n",
    "particle_lr = 1e-2\n",
    "lambda_reg = 1\n",
    "n_particles = 1000 # (num samples)\n",
    "sample_every = False\n",
    "noise_level = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rescale $\\phi^*$ by\n",
    "$$\n",
    "\\alpha = \\frac{\\text{SD}(\\phi^*)}{2 \\lambda \\Vert \\phi^* \\Vert_{L^2(q)}^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def phistar(params, _, x, aux=False):\n",
    "#     inducing_particles = params\n",
    "#     phi = stein.get_phistar(kernel, target.logpdf, inducing_particles)\n",
    "#     l2_phi_squared = utils.l2_norm(inducing_particles, phi)**2\n",
    "#     ksd = stein.stein_discrepancy(inducing_particles, target.logpdf, phi)\n",
    "#     alpha = ksd / (2*lambda_reg*l2_phi_squared)\n",
    "#     if aux:\n",
    "#         return -alpha*phi(x), {\"ksd\": ksd, \"alpha\": alpha}\n",
    "#     else:\n",
    "#         return -alpha*phi(x)\n",
    "\n",
    "\n",
    "# def phistar_with_bandwidth_heuristic(params, _, x, aux=False):\n",
    "#     inducing_particles = params\n",
    "#     bandwidth = kernels.median_heuristic(inducing_particles)\n",
    "#     kernel = kernels.get_rbf_kernel(bandwidth)\n",
    "\n",
    "#     phi = stein.get_phistar(kernel, target.logpdf, inducing_particles)\n",
    "#     l2_phi_squared = utils.l2_norm(inducing_particles, phi)**2\n",
    "#     ksd = stein.stein_discrepancy(inducing_particles, target.logpdf, phi)\n",
    "#     alpha = ksd / (2*lambda_reg*l2_phi_squared)\n",
    "#     if aux:\n",
    "#         return -alpha*phi(x), {\"ksd\": ksd, \"alpha\": alpha}\n",
    "#     else:\n",
    "#         return -alpha*phi(x)\n",
    "\n",
    "\n",
    "def phistar_batched(params, _, particles, aux=False):\n",
    "    kernel = kernels.get_rbf_kernel(1)\n",
    "    inducing_particles = params\n",
    "    phi = stein.get_phistar(kernel, target.logpdf, inducing_particles)\n",
    "    l2_phi_squared = utils.l2_norm(inducing_particles, phi)**2\n",
    "    ksd = stein.stein_discrepancy(inducing_particles, target.logpdf, phi)\n",
    "    alpha = ksd / (2*lambda_reg*l2_phi_squared)\n",
    "    if aux:\n",
    "        return -alpha*vmap(phi)(particles), {\"ksd\": ksd, \"alpha\": alpha}\n",
    "    else:\n",
    "        return -alpha*vmap(phi)(particles)\n",
    "\n",
    "\n",
    "def phistar_batched_adaptive(params, _, particles, aux=False):\n",
    "    inducing_particles = params\n",
    "    bandwidth = kernels.median_heuristic(inducing_particles)\n",
    "    kernel = kernels.get_rbf_kernel(bandwidth)\n",
    "    phi = stein.get_phistar(kernel, target.logpdf, inducing_particles)\n",
    "    l2_phi_squared = utils.l2_norm(inducing_particles, phi)**2\n",
    "    ksd = stein.stein_discrepancy(inducing_particles, target.logpdf, phi)\n",
    "    alpha = ksd / (2*lambda_reg*l2_phi_squared)\n",
    "    if aux:\n",
    "        return -alpha*vmap(phi)(particles), {\"ksd\": ksd, \"alpha\": alpha}\n",
    "    else:\n",
    "        return -alpha*vmap(phi)(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "stein_learner = models.SDLearner(subkey,\n",
    "                                 target=target,\n",
    "                                 sizes=sizes,\n",
    "                                 learning_rate=learning_rate,\n",
    "                                 lambda_reg=lambda_reg)\n",
    "\n",
    "# ksd_learner = models.KernelLearner(subkey,\n",
    "#                                   target,\n",
    "#                                   sizes=[2],\n",
    "#                                   activation_kernel=kernels.get_rbf_kernel(1),\n",
    "#                                   learning_rate=learning_rate,\n",
    "#                                   lambda_reg=lambda_reg,\n",
    "#                                   scaling_parameter=True)\n",
    "\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "particles_learned = models.Particles(subkey,\n",
    "                                 gradient=stein_learner.kl_gradient,\n",
    "                                 proposal=proposal,\n",
    "                                 n_particles=n_particles,\n",
    "                                 learning_rate=particle_lr,\n",
    "                                 only_training=True)\n",
    "\n",
    "particles_svgd = models.Particles(subkey,\n",
    "                                 gradient=phistar_batched,\n",
    "                                 proposal=proposal,\n",
    "                                 n_particles=n_particles,\n",
    "                                 learning_rate=particle_lr,\n",
    "                                 only_training=True)\n",
    "\n",
    "# particles_kernel_learned = models.Particles(subkey, # learn the KSD\n",
    "#                                  gradient=ksd_learner.kl_gradient,\n",
    "#                                  proposal=proposal,\n",
    "#                                  n_particles=n_particles,\n",
    "#                                  learning_rate=particle_lr,\n",
    "#                                  only_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVGD and SVGD without kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "n_steps=1000\n",
    "train_steps=5\n",
    "for _ in tqdm(range(n_steps)):\n",
    "    train_x, val_x = particles_learned.get_params(split_by_group=True)\n",
    "    x = stein_learner.train(train_x, val_x, key=subkey, n_steps=train_steps, noise_level=noise_level)\n",
    "    particles_learned.step(stein_learner.get_params())\n",
    "\n",
    "    inducing_particles, _ = particles_svgd.get_params(split_by_group=True)\n",
    "    particles_svgd.step(inducing_particles)\n",
    "\n",
    "#     train_x, _ = particles_kernel_learned.get_params(split_by_group=True)\n",
    "#     ksd_learner.train(train_x, n_steps=train_steps)\n",
    "#     particles_kernel_learned.step(ksd_learner.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=[20, 6])\n",
    "axs = axs.flatten()\n",
    "x_train_sl, single_val = particles_learned.get_params(split_by_group=True)\n",
    "x_train_svgd, _ = particles_svgd.get_params(split_by_group=True)\n",
    "# x_train_kernel_learned, _ = particles_kernel_learned.get_params(split_by_group=True)\n",
    "\n",
    "x_true = target.sample(n_particles)\n",
    "titles = [\"SVGD without kernels\", \"SVGD\", \"True samples\"]\n",
    "samples = (x_train_sl, x_train_svgd, x_true)\n",
    "\n",
    "xlims=(-10, 10)\n",
    "ylims=(-10, 25)\n",
    "\n",
    "\n",
    "\n",
    "for ax, x_train, title in zip(axs, samples, titles):\n",
    "    plot.scatter(x_train, ax=ax)\n",
    "#     plot.plot_fun_2d(target.pdf, type=\"contour\", ax=ax, xlims=xlims, ylims=ylims)\n",
    "    ax.set_ylim(ylims)\n",
    "    ax.set_xlim(xlims)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "fig, ax = plt.subplots(figsize=[16, 10])\n",
    "\n",
    "# scale_grid = np.arange(0, n_steps*10, step=10)\n",
    "val_sd = np.array(stein_learner.rundata[\"validation_sd\"])\n",
    "\n",
    "ax.plot(stein_learner.rundata[\"training_sd\"], \"--\", label=\"training SD\")\n",
    "# ax.plot(scale_grid, particles_svgd.rundata[\"ksd\"], label=\"SVGD SD\")\n",
    "ax.plot(val_sd[:,0], val_sd[:,1], \"--o\", label=\"validation_sd\")\n",
    "# ax.set_ylim((-1,1))\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[10,5])\n",
    "\n",
    "ylim1=None\n",
    "# ylim1 = (-10, 1.5)\n",
    "# ylim1 = (-0.5, 1.5)\n",
    "banana_mean = [0, 4]\n",
    "\n",
    "ax.plot(particles_learned.rundata[\"training_mean\"], label=\"SVGD-learned mean\", color=\"tab:blue\")\n",
    "for mean in banana_mean if isinstance(target, distributions.Banana) else target.mean:\n",
    "    ax.axhline(y=mean, linestyle=\"--\", color=\"green\")\n",
    "ax.plot(particles_svgd.rundata[\"training_mean\"], label=\"SVGD mean\", color=\"tab:orange\")\n",
    "ax.set_ylim(ylim1)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
