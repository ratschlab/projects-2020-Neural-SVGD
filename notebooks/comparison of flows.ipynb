{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/msc-thesis/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "from jax import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "# config.update(\"jax_disable_jit\", True)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/lauro/code/msc-thesis/svgd/kernel_learning/\")\n",
    "import json_tricks as json\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, random, lax, jacfwd, value_and_grad\n",
    "from jax import lax\n",
    "from jax.ops import index_update, index\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as onp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import haiku as hk\n",
    "import optax\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import config\n",
    "\n",
    "import utils\n",
    "import metrics\n",
    "import time\n",
    "import plot\n",
    "import stein\n",
    "import kernels\n",
    "import distributions\n",
    "import nets\n",
    "import models\n",
    "import flows\n",
    "\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "key, subkey = random.split(random.PRNGKey(0))\n",
    "from distributions import funnel, banana_target, ring_target, squiggle_target, mix_of_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 1.\n",
    "n_steps = 50\n",
    "n_particles = 50 * 1 # careful: are you splitting into train / val / test sets??\n",
    "setups = (funnel, banana_target, ring_target, squiggle_target, mix_of_gauss)\n",
    "sgld_hparams = {\n",
    "    \"particle_lr\": [1e-3, 5e-3, 1e-2, 5e-2, 1e-1],\n",
    "    \"particle_optimizer\": [\"sgd\", \"adam\"],\n",
    "}\n",
    "hparam_product = list(utils.dict_cartesian_product(**sgld_hparams))\n",
    "\n",
    "# for d in hparam_product:\n",
    "#     print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure:\n",
    "# 1) call flow(target, hparams, etc)\n",
    "# 2) compute final metrics on particles\n",
    "# 3) save rundata to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(hparams, setup):\n",
    "    \"Note: we're always using same random seed.\"\n",
    "    t = time.time()\n",
    "    startdate = time.strftime(\"%Y-%m-%d\", time.localtime(t))\n",
    "    starttime = time.strftime(\"%H:%M:%S\", time.localtime(t))\n",
    "    filename = f\"/home/lauro/code/msc-thesis/svgd/runs/sgld/{startdate}_{starttime}.json\"\n",
    "    target, _ = setup.get()\n",
    "    \n",
    "    # 1) call flow\n",
    "    gradient, particles, err =        flows.sgld_flow( subkey, setup, n_particles=n_particles, n_steps=n_steps, noise_level=noise_level, **hparams)\n",
    "#     gradient, particles, err =        flows.svgd_flow( subkey, setup, n_particles=n_particles, n_steps=n_steps, noise_level=noise_level, scaled=True)\n",
    "#     gradient, particles, err =        flows.score_flow(subkey, setup, n_particles=n_particles, n_steps=n_steps, noise_level=noise_level, scale=1.)\n",
    "\n",
    "#     gradient, particles, err = flows.neural_svgd_flow( subkey, setup, n_particles=n_particles, n_steps=n_steps, noise_level=noise_level, learner_lr=1e-3, patience=10)\n",
    "#     gradient, particles, err = flows.neural_score_flow(subkey, setup, n_particles=n_particles, n_steps=n_steps, noise_level=noise_level, learner_lr=1e-3, patience=10, lam=0)\n",
    "#     gradient, particles, err = flows.deep_kernel_flow( subkey, setup, n_particles=n_particles, n_steps=n_steps, noise_level=noise_level, learner_lr=1e-3, patience=3)\n",
    "\n",
    "    # 2) compute final metrics on particles\n",
    "    final_particles = particles.rundata['particles'][-1]\n",
    "    metrics_dict = metrics.compute_final_metrics(final_particles, target)\n",
    "\n",
    "    # 3) save rundata\n",
    "    data = {\n",
    "        \"hparams\": hparams,\n",
    "        \"particle_data\": particles.rundata,\n",
    "        \"gradient_data\": gradient.rundata,\n",
    "        \"final_metrics\": metrics_dict,\n",
    "    }\n",
    "    data = utils.dict_dejaxify(data)\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        filename = filename[:-5] + datetime.datetime.now().strftime(\".%f.json\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4, sort_keys=True, allow_nan=True)\n",
    "    return gradient, particles, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:14<00:00, 14.93s/it]\n"
     ]
    }
   ],
   "source": [
    "for setup in tqdm(setups):\n",
    "    for hparams in hparam_product:\n",
    "        gradient, particles, err = run(hparams, setup)\n",
    "        if err is not None:\n",
    "            raise err from None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/lauro/code/msc-thesis/svgd/runs/sgld/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-24-7c63f3185c71>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-7c63f3185c71>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    #     print(filename)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(directory):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
