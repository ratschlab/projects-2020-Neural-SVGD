{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgd import svgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accelerate the function `update` with `jit` we had to use the `static_argnums` argument. This is because `jit` only knows how to deal with functions that are 1) pure, that is no side effects, and 2) take only np arrays as inputs and outputs.\n",
    "\n",
    "In effect, the `static_argnums` argument tells `jit` to ignore the callable input `logp`. Everytime it changes, `jit` recompiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample\n",
    "## Experiment 1: one-dim Gaussian mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.1\n",
    "Target distribution:\n",
    "$$ p(x) = \\mathcal N(x; 0, 1) $$\n",
    "Initialization:\n",
    "$$ q_0(x) = \\mathcal N(x; -10, 1) $$\n",
    "Kernel bandwidth:\n",
    "$$ h = \\text{med}^2 / \\log n, $$\n",
    "where med is the median of the $ n \\choose 2$ pairwise distances between the points $x_1, \\dots, x_n$. Important to note: the bandwidth here changes adaptively across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm\n",
    "from svgd import kernel_param_update_rule\n",
    "\n",
    "@jit\n",
    "def logp(x):\n",
    "    \"\"\"\n",
    "    IN: single scalar np array x. alternatively, [x] works too\n",
    "    OUT: scalar logp(x)\n",
    "    \"\"\"\n",
    "    return np.squeeze(np.sum(norm.logpdf(x, loc=0, scale=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300\n",
    "stepsize = 0.001\n",
    "L = int(1 / stepsize)\n",
    "\n",
    "# generate data\n",
    "key = random.PRNGKey(6)\n",
    "x0 = random.normal(key, (n,1)) - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout, log = svgd(x0, logp, stepsize, L, kernel_param=None, kernel_param_update_rule=kernel_param_update_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2b82842bd409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m311\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2 plots on 0th axis, 1 plot on 1th axis, plot nr 1 --> 211\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Particle mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"particle_mean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m312\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.subplots_adjust(hspace=0.8)\n",
    "\n",
    "\n",
    "plt.subplot(311) # 2 plots on 0th axis, 1 plot on 1th axis, plot nr 1 --> 211\n",
    "plt.title(\"Particle mean\")\n",
    "plt.plot(log[\"particle_mean\"])\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title(\"Particle variance\")\n",
    "plt.plot(log[\"particle_var\"])\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title(\"Kernel parameter\")\n",
    "plt.plot(log[\"kernel_params\"])\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "_ = plt.hist(x0[:, 0])\n",
    "_ = plt.hist(xout[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[\"particle_var\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what speedup `jit` gets us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit svgd(x0, logp, stepsize, L, kernel_param=None, kernel_param_update_rule=kernel_param_update_rule).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With jitted `update`: 12.2 ms per loop\n",
    "* Without jitted `update`: 1.08 s per loop\n",
    "\n",
    "So that's a speedup of around 100x. Sweet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.2\n",
    "Target distribution:\n",
    "$$ p(x) = 1/3 \\mathcal N(-2, 1) + 2/3 \\mathcal N(2, 1) $$\n",
    "Initialization:\n",
    "$$ q_0(x) = \\mathcal N(-10, 1) $$\n",
    "Kernel bandwidth:\n",
    "$$ h = \\text{med}^2 / \\log n, $$\n",
    "where med is the median of the $ n \\choose 2$ pairwise distances between the points $x_1, \\dots, x_n$. Important to note: the bandwidth here changes adaptively across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm\n",
    "from svgd import kernel_param_update_rule\n",
    "\n",
    "def p(x):\n",
    "    \"\"\"\n",
    "    IN: single scalar np array x. alternatively, [x] works too\n",
    "    OUT: scalar logp(x)\n",
    "    \"\"\"\n",
    "    out = 1/3 * norm.pdf(x, loc=-2, scale=1) + 2/3 * norm.pdf(x, loc=2, scale=1)\n",
    "    return np.squeeze(out) # to be able to take a gradient, output must be scalar\n",
    "\n",
    "grid = np.arange(-5, 5, 0.1)\n",
    "_ = plt.plot(grid, p(grid))\n",
    "\n",
    "def logp(x):\n",
    "    return np.log(p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment:\n",
    "n = 100\n",
    "d = 1\n",
    "stepsize = 0.001\n",
    "L = int(1 / stepsize)\n",
    "\n",
    "# generate data\n",
    "key = random.PRNGKey(0)\n",
    "x0 = random.normal(key, (n,d)) - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout, log = svgd(x0, logp, stepsize, L, kernel_param = None, kernel_param_update_rule=kernel_param_update_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplots_adjust(hspace=1.2)\n",
    "\n",
    "\n",
    "plt.subplot(311) # 2 plots on 0th axis, 1 plot on 1th axis, plot nr 1 --> 211\n",
    "plt.title(\"Particle mean\")\n",
    "plt.plot(log[\"particle_mean\"])\n",
    "# plt.xlabel(\"Step\")\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title(\"Particle variance\")\n",
    "plt.plot(log[\"particle_var\"])\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title(\"Kernel parameter\")\n",
    "plt.plot(log[\"kernel_params\"])\n",
    "\n",
    "plt.figure(2)\n",
    "_ = plt.hist(x0[:, 0])\n",
    "_ = plt.hist(xout[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test functions\n",
    "mse1 = (np.mean(xout) - 2/3)**2\n",
    "mse2 = (np.mean(xout**2) - 5)**2\n",
    "print(mse1)\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Multi-dim Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.1\n",
    "Same as before, only now $X$ is $d$-dimensional.\n",
    "#### Target distribution:\n",
    "$$ p(x) = \\mathcal N(x; \\mu, \\Sigma), \\ \\ \\ \\text { where we take } \\mu = (1, \\dots, 1)^T \\text{ and } \\Sigma = I. $$\n",
    "#### Initialization:\n",
    "$$ q_0(x) = \\mathcal N(x; \\mu_0, \\Sigma_0), \\ \\ \\ \\text{ with } \\mu = (-10, \\dots, -10) \\text{ and } \\Sigma = I. $$\n",
    "\n",
    "#### Kernel:\n",
    "We'll use a natural generalization of the RBF kernel: $k(x, y) = \\exp(-\\frac{1}{2} (x - y)^T \\sigma^{-1} (x - y))$. The easiest way to adaptively choose the kernel is to let $\\Sigma = \\text{diag}(h_1, \\dots, h_d)$ be diagonal, in which case the kernel is given by $k(x, y) = \\exp(-\\frac{1}{2} \\sum_{k=1}^d \\frac{(x_k - y_k)^2}{h_k})$. Then choose the bandwidth parameter\n",
    "$$ h_k = \\text{med}_k^2 / \\log n, $$\n",
    "where med$_k$ is the $k$-th entry in the ($d$-dimensional) median of the $ n \\choose 2$ pairwise distances between the points $x_1, \\dots, x_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm\n",
    "from svgd import kernel_param_update_rule\n",
    "\n",
    "@jit\n",
    "def logp(x):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "    * x: np array of shape (d,)\n",
    "    \n",
    "    Returns: \n",
    "    * scalar log(p(x)), where p(x) is multidim gaussian\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1\n",
    "    return np.sum(norm.logpdf(x, loc=0, scale=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "d = 2\n",
    "stepsize = 0.001\n",
    "L = int(10 / stepsize)\n",
    "\n",
    "# generate data\n",
    "key = random.PRNGKey(0)\n",
    "x0 = random.normal(key, (n,d)) - 10\n",
    "\n",
    "xout, log = svgd(x0, logp, stepsize, L, kernel_param=None, kernel_param_update_rule=kernel_param_update_rule)\n",
    "if \"errors\" in log.keys():\n",
    "    print(log[\"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [8.0, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplots_adjust(hspace=1.2)\n",
    "log[\"particle_mean\"] = np.array(log[\"particle_mean\"])\n",
    "log[\"particle_var\"] = np.array(log[\"particle_var\"])\n",
    "log[\"kernel_params\"] = np.array(log[\"kernel_params\"])\n",
    "\n",
    "plt.subplot(311) # 2 plots on 0th axis, 1 plot on 1th axis, plot nr 1 --> 211\n",
    "plt.title(\"Particle mean\")\n",
    "_ = plt.plot(log[\"particle_mean\"])\n",
    "\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title(\"Particle variance\")\n",
    "_ = plt.plot(log[\"particle_var\"])\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title(\"Kernel parameter\")\n",
    "_ = plt.plot(log[\"kernel_params\"])\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [8.0, 4.8]\n",
    "plt.figure(2)\n",
    "_ = plt.hist(x0[:, 0])\n",
    "_ = plt.hist(xout[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(xout, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(xout, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Bayesian logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "n = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, $\\alpha \\sim \\text{Gamma}(a, b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import gamma\n",
    "\n",
    "# Generate regression parameters\n",
    "a = 2 # shape\n",
    "# b = 1 # rate parameter (1 / scale)\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "gamma_samples = random.gamma(key, a, shape=(1000,))\n",
    "\n",
    "count, bins, ignored = plt.hist(gamma_samples, bins = 100, density=True)\n",
    "y = gamma.pdf(bins, a)\n",
    "_ = plt.plot(bins, y, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(bins, 1 / y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "alpha = random.gamma(key, a, shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, $p(\\beta \\ \\vert \\alpha) = \\mathcal N (\\beta;\\ 0, \\alpha^{-1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = random.normal(key, shape=(2,)) * 1 / np.sqrt(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, $\\log \\frac{\\pi_i}{1 - \\pi_i} = \\beta_0 + \\beta_1 X_i$. In other words,\n",
    "$$ \\pi_i = \\frac{\\text{exp}(\\beta_0 + \\beta_1 X_i)}{1 - \\text{exp}(\\beta_0 + \\beta_1 X_i)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(start=-10, stop=10, num=n)\n",
    "pi = np.exp(beta[0] + beta[1] * x / (1 - np.exp(beta[0] + beta[1] * x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now sample the $Y_i$, which are distributed as\n",
    "$$Y_i \\ \\vert X_i \\sim \\text{Bernoulli}(\\pi_i).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = random.bernoulli(key, p=pi, shape=(n,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference\n",
    "We now have a set of observations $\\{(y_i, x_i)\\}_{i=1, \\dots, n}$, (that is, `x` and `y`) and a prior $p(\\beta_0, \\beta_1)$ over the logistic regression parameters.\n",
    "\n",
    "The prior is given as follows. Choose $\\beta_0, \\beta_1$ to be independently identically distributed as \n",
    "$$p(\\beta) = p(\\beta \\vert \\alpha) \\ p(\\alpha),$$\n",
    "where \n",
    "\\begin{align}   \n",
    "    p(\\beta \\vert \\alpha) &= \\mathcal N(\\beta; 0, \\alpha^{-1}) \\\\\n",
    "    p(\\alpha) &= \\text{Gamma}(\\alpha; a, b),\n",
    "\\end{align}\n",
    "and $a, b$ are fixed parameters.\n",
    "\n",
    "\n",
    "\n",
    "Then the posterior is given by\n",
    "\\begin{equation}\n",
    "    p(\\beta_0, \\beta_1 \\vert x, y) \\propto p(y \\vert x; \\beta_0, \\beta_1) p(\\beta_0, \\beta_1).\n",
    "\\end{equation}\n",
    "\n",
    "This posterior is the target distribution for SVGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(beta, alpha, x, y, a, b):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * beta: np array w/ shape (2,), regression parameters\n",
    "    * alpha: scalar > 0, precision parameter for beta prior\n",
    "    * x: np array of shape (n,)\n",
    "    * y: np array of shape (n,) and type bool (bernoulli sample)\n",
    "    * a, b: scalars > 0, parameters for gamma prior on alpha\n",
    "    \n",
    "    OUT:\n",
    "    * scalar, unnormalized posterior pdf evaluated at beta and alpha: p(beta, alpha | x, y, a, b)\n",
    "    \"\"\"\n",
    "    assert beta.shape[0] == 2\n",
    "    pi = np.exp(beta[0] + beta[1] * x) / (1 - np.exp(beta[0] + beta[1] * x))\n",
    "    \n",
    "    p1 = pi * y + (1 - pi) * (1 - y)\n",
    "    p1 = np.prod(p1) # likelihood of all x's\n",
    "    p2 = np.prod(norm.pdf(beta, loc=0, scale=1/alpha))\n",
    "    p3 = gamma.pdf(alpha, a)\n",
    "    \n",
    "    return p1 * p2 * p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = vmap(p, in_axes=(0, None, None, None, None, None))\n",
    "pbb = vmap(pb, in_axes=(0, None, None, None, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp(beta, alpha, x, y, a, b):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * beta: np array w/ shape (2,), regression parameters\n",
    "    * alpha: scalar > 0, precision parameter for beta prior\n",
    "    * x: np array of shape (n,)\n",
    "    * y: np array of shape (n,) and type bool (bernoulli sample)\n",
    "    * a, b: scalars > 0, parameters for gamma prior on alpha\n",
    "    \n",
    "    OUT:\n",
    "    * scalar, unnormalized posterior pdf evaluated at beta and alpha: p(beta, alpha | x, y, a, b)\n",
    "    \"\"\"\n",
    "    assert beta.shape[0] == 2\n",
    "    \n",
    "    log_pi = beta[0] + beta[1] * x - np.log(1 + np.exp(beta[0] + beta[1] * x)) # array of length n\n",
    "    one_minus_log_pi = np.log(1 + np.exp(beta[0] + beta[1] * x)) # array of length n\n",
    "    \n",
    "    log_p1 = y * log_pi + (1 - y) * one_minus_log_pi # array of length n\n",
    "    log_p1 = np.sum(log_p1) # scalar\n",
    "    \n",
    "    log_p2 = np.sum(norm.logpdf(beta, loc=0, scale=1/alpha))\n",
    "    log_p3 = gamma.logpdf(alpha, a)\n",
    "    \n",
    "    return log_p1 + log_p2 + log_p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logpb = vmap(logp, in_axes=(0, None, None, None, None, None))\n",
    "logpbb = vmap(logpb, in_axes=(0, None, None, None, None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick 3d plot of beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data\n",
    "num = 60\n",
    "X = np.linspace(-7, 7, num)\n",
    "Y = np.linspace(-7, 7, num)\n",
    "X, Y = np.meshgrid(X, Y) # both shape (40, 40)\n",
    "bet = np.array(list(zip(X, Y))).reshape(num, num, 2)\n",
    "\n",
    "b = np.array(1)\n",
    "print(logpbb(bet, alpha, x, y, a, b).shape)\n",
    "\n",
    "Z = logpbb(bet, alpha, x, y, a, b)\n",
    "Z = np.squeeze(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "plt.xlabel(\"beta_0\")\n",
    "plt.ylabel(\"beta_1\")\n",
    "ax.set_zlabel(\"p(beta_0, beta_1, alpha)\")\n",
    "\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
