{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgd import svgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accelerate the function `update` with `jit` we had to use the `static_argnums` argument. This is because `jit` only knows how to deal with functions that are 1) pure, that is no side effects, and 2) take only np arrays as inputs and outputs.\n",
    "\n",
    "In effect, the `static_argnums` argument tells `jit` to ignore the callable input `logp`. Everytime it changes, `jit` recompiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample\n",
    "## Experiment 1: one-dim Gaussian mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.1\n",
    "Target distribution:\n",
    "$$ p(x) = \\mathcal N(x; 0, 1) $$\n",
    "Initialization:\n",
    "$$ q_0(x) = \\mathcal N(x; -10, 1) $$\n",
    "Kernel bandwidth:\n",
    "$$ h = \\text{med}^2 / \\log n, $$\n",
    "where med is the median of the $ n \\choose 2$ pairwise distances between the points $x_1, \\dots, x_n$. Important to note: the bandwidth here changes adaptively across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm\n",
    "from svgd import kernel_param_update_rule\n",
    "\n",
    "@jit\n",
    "def logp(x):\n",
    "    \"\"\"\n",
    "    IN: single scalar np array x. alternatively, [x] works too\n",
    "    OUT: scalar logp(x)\n",
    "    \"\"\"\n",
    "    return np.squeeze(np.sum(norm.logpdf(x, loc=0, scale=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "stepsize = 0.001\n",
    "L = int(1 / stepsize / 3)\n",
    "\n",
    "# generate data\n",
    "key = random.PRNGKey(6)\n",
    "x0 = random.normal(key, (n,1)) - 10\n",
    "\n",
    "xout, log = svgd(x0, logp, stepsize, L, kernel_param=None, kernel_param_update_rule=kernel_param_update_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplots_adjust(hspace=0.8)\n",
    "\n",
    "\n",
    "plt.subplot(311) # 2 plots on 0th axis, 1 plot on 1th axis, plot nr 1 --> 211\n",
    "plt.title(\"Particle mean\")\n",
    "plt.plot(log[\"particle_mean\"])\n",
    "plt.xlabel(\"Step\")\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title(\"Particle variance\")\n",
    "plt.plot(log[\"particle_var\"])\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title(\"Kernel parameter\")\n",
    "plt.plot(log[\"kernel_params\"])\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "_ = plt.hist(x0[:, 0])\n",
    "_ = plt.hist(xout[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what speedup `jit` gets us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit svgd(x0, logp, stepsize, L, kernel_param=None, kernel_param_update_rule=kernel_param_update_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With jitted `update`: 12.2 ms per loop\n",
    "* Without jitted `update`: 1.08 s per loop\n",
    "\n",
    "So that's a speedup of around 100x. Sweet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.2\n",
    "Target distribution:\n",
    "$$ p(x) = 1/3 \\mathcal N(-2, 1) + 2/3 \\mathcal N(2, 1) $$\n",
    "Initialization:\n",
    "$$ q_0(x) = \\mathcal N(-10, 1) $$\n",
    "Kernel bandwidth:\n",
    "$$ h = \\text{med}^2 / \\log n, $$\n",
    "where med is the median of the $ n \\choose 2$ pairwise distances between the points $x_1, \\dots, x_n$. Important to note: the bandwidth here changes adaptively across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm\n",
    "from svgd import kernel_param_update_rule\n",
    "\n",
    "def p(x):\n",
    "    \"\"\"\n",
    "    IN: single scalar np array x. alternatively, [x] works too\n",
    "    OUT: scalar logp(x)\n",
    "    \"\"\"\n",
    "    out = 1/3 * norm.pdf(x, loc=-2, scale=1) + 2/3 * norm.pdf(x, loc=2, scale=1)\n",
    "    return np.squeeze(out) # to be able to take a gradient, output must be scalar\n",
    "\n",
    "grid = np.arange(-5, 5, 0.1)\n",
    "_ = plt.plot(grid, p(grid))\n",
    "\n",
    "def logp(x):\n",
    "    return np.log(p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment:\n",
    "n = 100\n",
    "d = 1\n",
    "stepsize = 0.001\n",
    "L = int(1 / stepsize)\n",
    "\n",
    "# generate data\n",
    "key = random.PRNGKey(0)\n",
    "x0 = random.normal(key, (n,d)) - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout, log = svgd(x0, logp, stepsize, L, kernel_param = None, kernel_param_update_rule=kernel_param_update_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplots_adjust(hspace=1.2)\n",
    "\n",
    "\n",
    "plt.subplot(311) # 2 plots on 0th axis, 1 plot on 1th axis, plot nr 1 --> 211\n",
    "plt.title(\"Particle mean\")\n",
    "plt.plot(log[\"particle_mean\"])\n",
    "# plt.xlabel(\"Step\")\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title(\"Particle variance\")\n",
    "plt.plot(log[\"particle_var\"])\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title(\"Kernel parameter\")\n",
    "plt.plot(log[\"kernel_params\"])\n",
    "\n",
    "plt.figure(2)\n",
    "_ = plt.hist(x0[:, 0])\n",
    "_ = plt.hist(xout[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test functions\n",
    "mse1 = (np.mean(xout) - 2/3)**2\n",
    "mse2 = (np.mean(xout**2) - 5)**2\n",
    "print(mse1)\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Multi-dim Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.1\n",
    "Same as before, only now $X$ is $d$-dimensional.\n",
    "#### Target distribution:\n",
    "$$ p(x) = \\mathcal N(x; \\mu, \\Sigma), \\ \\ \\ \\text { where we take } \\mu = (1, \\dots, 1)^T \\text{ and } \\Sigma = I. $$\n",
    "#### Initialization:\n",
    "$$ q_0(x) = \\mathcal N(x; \\mu_0, \\Sigma_0), \\ \\ \\ \\text{ with } \\mu = (-10, \\dots, -10) \\text{ and } \\Sigma = I. $$\n",
    "\n",
    "#### Kernel:\n",
    "We'll use a natural generalization of the RBF kernel: $k(x, y) = \\exp(-\\frac{1}{2} (x - y)^T \\sigma^{-1} (x - y))$. The easiest way to adaptively choose the kernel is to let $\\Sigma = \\text{diag}(h_1, \\dots, h_d)$ be diagonal, in which case the kernel is given by $k(x, y) = \\exp(-\\frac{1}{2} \\sum_{k=1}^d \\frac{(x_k - y_k)^2}{h_k})$. Then choose the bandwidth parameter\n",
    "$$ h_k = \\text{med}_k^2 / \\log n, $$\n",
    "where med$_k$ is the $k$-th entry in the ($d$-dimensional) median of the $ n \\choose 2$ pairwise distances between the points $x_1, \\dots, x_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm\n",
    "\n",
    "@jit\n",
    "def logp(x):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "    * x: np array of shape (d,)\n",
    "    \n",
    "    Returns: \n",
    "    * scalar log(p(x)), where p(x) is multidim gaussian\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1\n",
    "    return np.squeeze(np.sum(norm.logpdf(x, loc=0, scale=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgd import kernel_param_update_rule\n",
    "vkernel_param_update_rule = vmap(kernel_param_update_rule, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "d = 3\n",
    "stepsize = 0.001\n",
    "L = int(1 / stepsize )\n",
    "\n",
    "# generate data\n",
    "key = random.PRNGKey(6)\n",
    "x0 = random.normal(key, (n,d)) - 10\n",
    "\n",
    "xout, log = svgd(x0, logp, stepsize, L, kernel_param=None, kernel_param_update_rule=vkernel_param_update_rule)\n",
    "if \"errors\" in log.keys():\n",
    "    print(log[\"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = [8.0, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplots_adjust(hspace=1.2)\n",
    "log[\"particle_mean\"] = np.array(log[\"particle_mean\"])\n",
    "log[\"particle_var\"] = np.array(log[\"particle_var\"])\n",
    "log[\"kernel_params\"] = np.array(log[\"kernel_params\"])\n",
    "\n",
    "plt.subplot(311) # 2 plots on 0th axis, 1 plot on 1th axis, plot nr 1 --> 211\n",
    "plt.title(\"Particle mean\")\n",
    "_ = plt.plot(log[\"particle_mean\"])\n",
    "# plt.xlabel(\"Step\")\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title(\"Particle variance\")\n",
    "_ = plt.plot(log[\"particle_var\"])\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title(\"Kernel parameter\")\n",
    "_ = plt.plot(log[\"kernel_params\"])\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [8.0, 4.8]\n",
    "plt.figure(2)\n",
    "_ = plt.hist(x0[:, 0])\n",
    "_ = plt.hist(xout[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(xout, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(xout, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Bayesian logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "n = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, $\\alpha \\sim \\text{Gamma}(a, b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import gamma\n",
    "\n",
    "# Generate regression parameters\n",
    "a = 2 # shape\n",
    "# b = 1 # rate parameter (1 / scale)\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "gamma_samples = random.gamma(key, a, shape=(1000,))\n",
    "\n",
    "count, bins, ignored = plt.hist(gamma_samples, bins = 100, density=True)\n",
    "y = gamma.pdf(bins, a)\n",
    "_ = plt.plot(bins, y, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(bins, 1 / y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "alpha = random.gamma(key, a, shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, $p(\\beta \\ \\vert \\alpha) = \\mathcal N (\\beta;\\ 0, \\alpha^{-1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = random.normal(key, shape=(2,)) * 1 / np.sqrt(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, $\\log \\frac{\\pi_i}{1 - \\pi_i} = \\beta_0 + \\beta_1 X_i$. In other words,\n",
    "$$ \\pi_i = \\frac{\\text{exp}(\\beta_0 + \\beta_1 X_i)}{1 - \\text{exp}(\\beta_0 + \\beta_1 X_i)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(start=-10, stop=10, num=n)\n",
    "pi = np.exp(beta[0] + beta[1] * x / (1 - np.exp(beta[0] + beta[1] * x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now sample the $Y_i$, which are distributed as\n",
    "$$Y_i \\ \\vert X_i \\sim \\text{Bernoulli}(\\pi_i).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = random.bernoulli(key, p=pi, shape=(n,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference\n",
    "We now have a set of observations $\\{(y_i, x_i)\\}_{i=1, \\dots, n}$, (that is, `x` and `y`) and a prior $p(\\beta_0, \\beta_1)$ over the logistic regression parameters.\n",
    "\n",
    "The prior is given as follows. Choose $\\beta_0, \\beta_1$ to be independently identically distributed as \n",
    "$$p(\\beta) = p(\\beta \\vert \\alpha) \\ p(\\alpha),$$\n",
    "where \n",
    "\\begin{align}   \n",
    "    p(\\beta \\vert \\alpha) &= \\mathcal N(\\beta; 0, \\alpha^{-1}) \\\\\n",
    "    p(\\alpha) &= \\text{Gamma}(\\alpha; a, b),\n",
    "\\end{align}\n",
    "and $a, b$ are fixed parameters.\n",
    "\n",
    "\n",
    "\n",
    "Then the posterior is given by\n",
    "\\begin{equation}\n",
    "    p(\\beta_0, \\beta_1 \\vert x, y) \\propto p(y \\vert x; \\beta_0, \\beta_1) p(\\beta_0, \\beta_1).\n",
    "\\end{equation}\n",
    "\n",
    "This posterior is the target distribution for SVGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(beta, alpha, x, y, a, b):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * beta: np array w/ shape (2,), regression parameters\n",
    "    * alpha: scalar > 0, precision parameter for beta prior\n",
    "    * x: np array of shape (n,)\n",
    "    * y: np array of shape (n,) and type bool (bernoulli sample)\n",
    "    * a, b: scalars > 0, parameters for gamma prior on alpha\n",
    "    \n",
    "    OUT:\n",
    "    * scalar, unnormalized posterior pdf evaluated at beta and alpha: p(beta, alpha | x, y, a, b)\n",
    "    \"\"\"\n",
    "    assert beta.shape[0] == 2\n",
    "    pi = np.exp(beta[0] + beta[1] * x) / (1 - np.exp(beta[0] + beta[1] * x))\n",
    "    \n",
    "    p1 = pi * y + (1 - pi) * (1 - y)\n",
    "    p1 = np.prod(p1) # likelihood of all x's\n",
    "    p2 = np.prod(norm.pdf(beta, loc=0, scale=1/alpha))\n",
    "    p3 = gamma.pdf(alpha, a)\n",
    "    \n",
    "    return p1 * p2 * p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = vmap(p, in_axes=(0, None, None, None, None, None))\n",
    "pbb = vmap(pb, in_axes=(0, None, None, None, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp(beta, alpha, x, y, a, b):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * beta: np array w/ shape (2,), regression parameters\n",
    "    * alpha: scalar > 0, precision parameter for beta prior\n",
    "    * x: np array of shape (n,)\n",
    "    * y: np array of shape (n,) and type bool (bernoulli sample)\n",
    "    * a, b: scalars > 0, parameters for gamma prior on alpha\n",
    "    \n",
    "    OUT:\n",
    "    * scalar, unnormalized posterior pdf evaluated at beta and alpha: p(beta, alpha | x, y, a, b)\n",
    "    \"\"\"\n",
    "    assert beta.shape[0] == 2\n",
    "    \n",
    "    log_pi = beta[0] + beta[1] * x - np.log(1 + np.exp(beta[0] + beta[1] * x)) # array of length n\n",
    "    one_minus_log_pi = np.log(1 + np.exp(beta[0] + beta[1] * x)) # array of length n\n",
    "    \n",
    "    log_p1 = y * log_pi + (1 - y) * one_minus_log_pi # array of length n\n",
    "    log_p1 = np.sum(log_p1) # scalar\n",
    "    \n",
    "    log_p2 = np.sum(norm.logpdf(beta, loc=0, scale=1/alpha))\n",
    "    log_p3 = gamma.logpdf(alpha, a)\n",
    "    \n",
    "    return log_p1 + log_p2 + log_p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logpb = vmap(logp, in_axes=(0, None, None, None, None, None))\n",
    "logpbb = vmap(logpb, in_axes=(0, None, None, None, None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick 3d plot of beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make data\n",
    "num = 60\n",
    "X = np.linspace(-7, 7, num)\n",
    "Y = np.linspace(-7, 7, num)\n",
    "X, Y = np.meshgrid(X, Y) # both shape (40, 40)\n",
    "bet = np.array(list(zip(X, Y))).reshape(num, num, 2)\n",
    "\n",
    "b = np.array(1)\n",
    "print(logpbb(bet, alpha, x, y, a, b).shape)\n",
    "\n",
    "Z = logpbb(bet, alpha, x, y, a, b)\n",
    "Z = np.squeeze(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "plt.xlabel(\"beta_0\")\n",
    "plt.ylabel(\"beta_1\")\n",
    "ax.set_zlabel(\"p(beta_0, beta_1, alpha)\")\n",
    "\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
