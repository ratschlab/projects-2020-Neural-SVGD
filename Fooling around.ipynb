{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "from jax import lax\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare times for jitted svgd: fixed param vs not fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgd import svgd, fixed_param_svgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/msc-thesis/lib/python3.6/site-packages/jax/lib/xla_bridge.py:122: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "from jax.scipy.stats import norm\n",
    "from svgd import kernel_param_update_rule\n",
    "\n",
    "@jit\n",
    "def logp(x):\n",
    "    \"\"\"\n",
    "    IN: single scalar np array x. alternatively, [x] works too\n",
    "    OUT: scalar logp(x)\n",
    "    \"\"\"\n",
    "    return np.squeeze(np.sum(norm.logpdf(x, loc=0, scale=1)))\n",
    "\n",
    "n = 10\n",
    "stepsize = 0.01\n",
    "L = int(1 / stepsize)\n",
    "\n",
    "# generate data\n",
    "key = random.PRNGKey(1)\n",
    "x = random.normal(key, (n,1)) - 10\n",
    "\n",
    "kernel_param = kernel_param_update_rule(x)\n",
    "hfun = lambda x: kernel_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First just once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 µs ± 65.1 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit svgd(x, logp, stepsize, L, hfun).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 µs ± 27.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fixed_param_svgd(x, logp, stepsize, L, kernel_param).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now see what happens if we sweep over a grid of `kernel_param` values. First, define param search using regular svgd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_mse(logp, n, stepsize, L, m, q, kernel_param_update_rule=None):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * logp: callable, computes log(p(x)).\n",
    "    * n: integer, number of particles\n",
    "    * stepsize: float\n",
    "    * L: integer, number of SVGD steps\n",
    "    * m: integer, number of samples for averaging MSE\n",
    "    * q: callable, takes as argument a seed key and outputs samples of initial distributio q0\n",
    "    * kernel_param_update_rule: callable, takes the updated particles as input and outputs an updated set of kernel parameters.\n",
    "    If kernel_param constant, set to lambda x: const\n",
    "\n",
    "    OUT:\n",
    "    * dictionary of MSE values\n",
    "\n",
    "    \"\"\"\n",
    "    mse1 = []\n",
    "    mse2 = []\n",
    "    mse3 = []\n",
    "    for seed in range(0, m):\n",
    "        key = random.PRNGKey(seed)\n",
    "        x = q(key, n)\n",
    "\n",
    "        xout = svgd(x, logp, stepsize, L, kernel_param_update_rule)\n",
    "        mse1.append((np.mean(xout) - 2/3)**2)\n",
    "        mse2.append((np.mean(xout**2) - 5)**2)\n",
    "\n",
    "        w = random.normal(key, (1,))\n",
    "        mse3.append((np.mean(np.cos(w * xout) - np.exp(-w**2 / 2))**2))\n",
    "\n",
    "    mse1 = np.mean(np.array(mse1))\n",
    "    mse2 = np.mean(np.array(mse2))\n",
    "    mse3 = np.mean(np.array(mse3))\n",
    "\n",
    "    mse = {\n",
    "    \"E[x]\": mse1,\n",
    "    \"E[x^2]\": mse2,\n",
    "    \"E[cos(wx)]\": mse3\n",
    "    }\n",
    "    \n",
    "    return mse\n",
    "\n",
    "default_q = lambda key, n: random.normal(key, shape=(n,1)) - 10\n",
    "def kernel_param_search(logp, n, stepsize, L, m, kernel_param_grid, q=default_q):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * logp: callable, computes log(p(x)).\n",
    "    * n: integer, number of particles\n",
    "    * stepsize: float\n",
    "    * L: integer, number of SVGD steps\n",
    "    * kernel_param_grid: one-dimensional np array\n",
    "    * m: integer, number of samples for computing MSE\n",
    "    * q: callable, takes as argument a seed key and outputs samples of initial distributio q0\n",
    "\n",
    "    \n",
    "    OUT:\n",
    "    * dictionary consisting of three np arrays of the same length as kernel_param_grid. Entries are MSE values.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    mse1s = []\n",
    "    mse2s = []\n",
    "    mse3s = []\n",
    "\n",
    "    for h in kernel_param_grid:\n",
    "        mse1, mse2, mse3 = list(get_mse(logp, n, stepsize, L, m, q, lambda x: h).values())\n",
    "        \n",
    "        mse1s.append(mse1)\n",
    "        mse2s.append(mse2)\n",
    "        mse3s.append(mse3)\n",
    "    \n",
    "    mses = {\n",
    "        \"E[x]\": mse1s,\n",
    "        \"E[x^2]\": mse2s,\n",
    "        \"E[cos(wx)]\": mse3s\n",
    "    }\n",
    "    \n",
    "    return mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_mse_fixed_param(logp, n, stepsize, L, m, q, kernel_param):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * logp: callable, computes log(p(x)).\n",
    "    * n: integer, number of particles\n",
    "    * stepsize: float\n",
    "    * L: integer, number of SVGD steps\n",
    "    * m: integer, number of samples for averaging MSE\n",
    "    * q: callable, takes as argument a seed key and outputs samples of initial distributio q0\n",
    "\n",
    "    OUT:\n",
    "    * dictionary of MSE values\n",
    "\n",
    "    \"\"\"\n",
    "    mse1 = []\n",
    "    mse2 = []\n",
    "    mse3 = []\n",
    "    for seed in range(0, m):\n",
    "        key = random.PRNGKey(seed)\n",
    "        x = q(key, n)\n",
    "\n",
    "        xout = fixed_param_svgd(x, logp, stepsize, L, kernel_param)\n",
    "        mse1.append((np.mean(xout) - 2/3)**2)\n",
    "        mse2.append((np.mean(xout**2) - 5)**2)\n",
    "\n",
    "        w = random.normal(key, (1,))\n",
    "        mse3.append((np.mean(np.cos(w * xout) - np.exp(-w**2 / 2))**2))\n",
    "\n",
    "    mse1 = np.mean(np.array(mse1))\n",
    "    mse2 = np.mean(np.array(mse2))\n",
    "    mse3 = np.mean(np.array(mse3))\n",
    "\n",
    "    mse = {\n",
    "    \"E[x]\": mse1,\n",
    "    \"E[x^2]\": mse2,\n",
    "    \"E[cos(wx)]\": mse3\n",
    "    }\n",
    "    \n",
    "    return mse\n",
    "\n",
    "default_q = lambda key, n: random.normal(key, shape=(n,1)) - 10\n",
    "def fixed_kernel_param_search(logp, n, stepsize, L, m, kernel_param_grid, q=default_q):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * logp: callable, computes log(p(x)).\n",
    "    * n: integer, number of particles\n",
    "    * stepsize: float\n",
    "    * L: integer, number of SVGD steps\n",
    "    * kernel_param_grid: one-dimensional np array\n",
    "    * m: integer, number of samples for computing MSE\n",
    "    * q: callable, takes as argument a seed key and outputs samples of initial distributio q0\n",
    "\n",
    "    \n",
    "    OUT:\n",
    "    * dictionary consisting of three np arrays of the same length as kernel_param_grid. Entries are MSE values.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    mse1s = []\n",
    "    mse2s = []\n",
    "    mse3s = []\n",
    "\n",
    "    for h in kernel_param_grid:\n",
    "        mse1, mse2, mse3 = list(get_mse_fixed_param(logp, n, stepsize, L, m, q, h).values())\n",
    "        \n",
    "        mse1s.append(mse1)\n",
    "        mse2s.append(mse2)\n",
    "        mse3s.append(mse3)\n",
    "    \n",
    "    mses = {\n",
    "        \"E[x]\": mse1s,\n",
    "        \"E[x^2]\": mse2s,\n",
    "        \"E[cos(wx)]\": mse3s\n",
    "    }\n",
    "    \n",
    "    return mses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "stepsize = 0.01\n",
    "L = int(1 / stepsize)\n",
    "kernel_param_grid = np.logspace(-10, 15, num=25, base=2) # params smaller than 2^10 are generally awful\n",
    "m = 10\n",
    "\n",
    "q = lambda key, n: random.normal(key, shape=(n,1)) - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.6034951210022\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mses = kernel_param_search(logp, n, stepsize, L, m, kernel_param_grid, q)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4158875942230225\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mses_f = fixed_kernel_param_search(logp, n, stepsize, L, m, kernel_param_grid, q=default_q)\n",
    "elapsed_time_f = time.time() - start_time\n",
    "print(elapsed_time_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: inefficiency comes from recompiling every time for a new `kernel_param`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the question is: did I just add this inefficiency while `jit`ing `svgd`? Or was it already there? Let's check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def old_svgd(x, logp, stepsize, L, kernel_param_update_rule):\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from svgd import update\n",
    "def old_svgd(x, logp, stepsize, L, kernel_param_update_rule):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * x is an np array of shape n x d\n",
    "    * logp is the log of a differentiable pdf p (callable)\n",
    "    * stepsize is a float\n",
    "    * kernel_param is a positive scalar: bandwidth parameter for RBF kernel\n",
    "    * L is an integer (number of iterations)\n",
    "    * kernel_param_update_rule is a callable that takes the updated particles as input and outputs an updated set of kernel parameters. If supplied, the argument kernel_param will be ignored.\n",
    "\n",
    "    OUT:\n",
    "    * Updated particles x (np array of shape n x d) after L steps of SVGD\n",
    "    * dictionary with logs\n",
    "    \"\"\"\n",
    "    assert x.ndim == 2\n",
    "\n",
    "    log = {\n",
    "        \"kernel_param\": [],\n",
    "        \"particle_mean\": [],\n",
    "        \"particle_var\": []\n",
    "    }\n",
    "\n",
    "    \n",
    "    for i in range(L):\n",
    "        kernel_param = kernel_param_update_rule(x)\n",
    "        x = update(x, logp, stepsize, kernel_param)\n",
    "\n",
    "        update_dict = {\n",
    "            \"kernel_param\": kernel_param,\n",
    "            \"particle_mean\": np.mean(x, axis=0),\n",
    "            \"particle_var\": np.var(x, axis=0)\n",
    "        }\n",
    "\n",
    "        for key in log.keys():\n",
    "            log[key].append(update_dict[key])\n",
    "            \n",
    "    for key in log.keys():\n",
    "        log[key] = np.array(log[key])\n",
    "\n",
    "    return x#, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 ms ± 7.27 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit old_svgd(x, logp, stepsize, L, hfun).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So seems the old SVGD was around 1000 times slower not counting recompiles. Note that `%timeit` assumes no recompiling. Let's see what happens if we define an `old_kernel_param_search`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_mse_old(logp, n, stepsize, L, m, q, kernel_param_fun):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * logp: callable, computes log(p(x)).\n",
    "    * n: integer, number of particles\n",
    "    * stepsize: float\n",
    "    * L: integer, number of SVGD steps\n",
    "    * m: integer, number of samples for averaging MSE\n",
    "    * q: callable, takes as argument a seed key and outputs samples of initial distributio q0\n",
    "\n",
    "    OUT:\n",
    "    * dictionary of MSE values\n",
    "\n",
    "    \"\"\"\n",
    "    mse1 = []\n",
    "    mse2 = []\n",
    "    mse3 = []\n",
    "    for seed in range(0, m):\n",
    "        key = random.PRNGKey(seed)\n",
    "        x = q(key, n)\n",
    "\n",
    "        xout = old_svgd(x, logp, stepsize, L, kernel_param_fun)\n",
    "        mse1.append((np.mean(xout) - 2/3)**2)\n",
    "        mse2.append((np.mean(xout**2) - 5)**2)\n",
    "\n",
    "        w = random.normal(key, (1,))\n",
    "        mse3.append((np.mean(np.cos(w * xout) - np.exp(-w**2 / 2))**2))\n",
    "\n",
    "    mse1 = np.mean(np.array(mse1))\n",
    "    mse2 = np.mean(np.array(mse2))\n",
    "    mse3 = np.mean(np.array(mse3))\n",
    "\n",
    "    mse = {\n",
    "    \"E[x]\": mse1,\n",
    "    \"E[x^2]\": mse2,\n",
    "    \"E[cos(wx)]\": mse3\n",
    "    }\n",
    "    \n",
    "    return mse\n",
    "\n",
    "default_q = lambda key, n: random.normal(key, shape=(n,1)) - 10\n",
    "def old_kernel_param_search(logp, n, stepsize, L, m, kernel_param_grid, q=default_q):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "    * logp: callable, computes log(p(x)).\n",
    "    * n: integer, number of particles\n",
    "    * stepsize: float\n",
    "    * L: integer, number of SVGD steps\n",
    "    * kernel_param_grid: one-dimensional np array\n",
    "    * m: integer, number of samples for computing MSE\n",
    "    * q: callable, takes as argument a seed key and outputs samples of initial distributio q0\n",
    "\n",
    "    \n",
    "    OUT:\n",
    "    * dictionary consisting of three np arrays of the same length as kernel_param_grid. Entries are MSE values.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    mse1s = []\n",
    "    mse2s = []\n",
    "    mse3s = []\n",
    "\n",
    "    for h in kernel_param_grid:\n",
    "        mse1, mse2, mse3 = list(get_mse_old(logp, n, stepsize, L, m, q, lambda x: h).values())\n",
    "        \n",
    "        mse1s.append(mse1)\n",
    "        mse2s.append(mse2)\n",
    "        mse3s.append(mse3)\n",
    "    \n",
    "    mses = {\n",
    "        \"E[x]\": mse1s,\n",
    "        \"E[x^2]\": mse2s,\n",
    "        \"E[cos(wx)]\": mse3s\n",
    "    }\n",
    "    \n",
    "    return mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.18507409095764\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mses_o = old_kernel_param_search(logp, n, stepsize, L, m, kernel_param_grid, q=default_q)\n",
    "elapsed_time_o = time.time() - start_time\n",
    "print(elapsed_time_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is around a third of the svgd version that recompiles at every new kernel param. This is consistent with the above hypothesis, since `old_svgd` here only recompiles every `m` runs when a new kernel param is tried. So hypothesis: `elapsed_time` / `elapsed_time_o` is circa equal to `m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0.8202781639399668\n"
     ]
    }
   ],
   "source": [
    "print(m)\n",
    "print(elapsed_time / elapsed_time_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damn, hypothesis discomfirmed (`m`=10, random seed `PRNGKey(1)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# another `fori_loop` test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fun(i, lis):\n",
    "    lis.append(i)\n",
    "    del lis[0]\n",
    "    return lis\n",
    "\n",
    "lax.fori_loop(0, 10, update_fun, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_fori_loop(0, 10, update_fun, [1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `lax.scan`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough python equivalent:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(f, init, xs, length=None):\n",
    "    if xs is None:\n",
    "        xs = [None] * length\n",
    "    carry = init\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        carry, y = f(carry, x)\n",
    "        ys.append(y)\n",
    "    return carry, np.stack(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we define `fori_loop` in terms of `scan`? Code taken from a github issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiable_fori_loop(lower, upper, body_fun, init_val):\n",
    "    f = lambda x, i: (body_fun(i, x), ())\n",
    "    result, _ = lax.scan(f, init_val, np.arange(lower, upper))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_fori_loop(lower, upper, body_fun, init_val):\n",
    "    val = init_val\n",
    "    for i in range(lower, upper):\n",
    "        val = body_fun(i, val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda i, x: x**2 + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lax.fori_loop(0, 3, f, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_fori_loop(0, 3, f, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differentiable_fori_loop(0, 3, f, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fori loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.lax import fori_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def test(L):\n",
    "    return fori_loop(0, L, lambda i, n: n+1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare a normal for loop, where we cant `jit` the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "def test2(L):\n",
    "    x = 1 # init_val\n",
    "    for _ in range(L):\n",
    "        x = x + 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using `grad` with `fori_loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test3(x):\n",
    "    \"\"\"output x^4 using fori loop to compute\"\"\"\n",
    "    return fori_loop(0, 2, lambda i, y: y*y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3(3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(test3)(3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd\n",
    "jacfwd(test3)(3.) # 4*x^3 = 4 * 27 = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpu vs cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.lib import xla_bridge\n",
    "backend = xla_bridge.get_backend().platform\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "d = 10000\n",
    "x = random.normal(key, shape=(d, d))\n",
    "key = random.split(key, 1)[0]\n",
    "y = random.normal(key, shape=(d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running on {backend}.\")\n",
    "print()\n",
    "%timeit np.matmul(x, y).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `np.sort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[2,3,4],\n",
    "              [3,2,1]])\n",
    "print(x.shape)\n",
    "np.sort(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# can `jit` take care of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_vdot = vmap(np.vdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x, y):\n",
    "    \"\"\"\n",
    "    do a lot of useless repetitive stuff. return np.vdot(x, y)\n",
    "    \"\"\"\n",
    "    n = 2 * 10**6\n",
    "    xtiled = np.tile(x, (n, 1))\n",
    "    ytiled = np.tile(y, (n, 1))\n",
    "    out = batched_vdot(xtiled, ytiled)\n",
    "    return out[0]\n",
    "\n",
    "jt = jit(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,1,1])\n",
    "y = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit test(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit jt(x, y).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jup, it can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quicktest(x, y):\n",
    "    n = 2\n",
    "    xtiled = np.tile(x, (n, 1))\n",
    "    ytiled = np.tile(y, (n, 1))\n",
    "    out = batched_vdot(xtiled, ytiled)\n",
    "    return out[0]\n",
    "\n",
    "qjt = jit(quicktest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit quicktest(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit qjt(x, y).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import single_rbf, ard\n",
    "for x in np.linspace(-10, 10, 30):\n",
    "    x = np.array([x])\n",
    "    for y in np.linspace(-10, 10, 30):\n",
    "        y = np.array([y])\n",
    "        for h in np.linspace(1, 100, 5):\n",
    "#             print(\"x = \", x)\n",
    "#             print(\"y = \", y)\n",
    "#             print()\n",
    "#             print(\"rbf(x, y): \", single_rbf(x, y, h))\n",
    "#             print(\"ard(x, y): \", ard(x, y, h))\n",
    "#             print()\n",
    "#             print(\"-----------\")\n",
    "            assert single_rbf(x, y, h) == ard(x, y, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misc jax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply matrices\n",
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=np.float32)\n",
    "%timeit np.dot(x, x.T).block_until_ready()  # runs on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp  # original CPU-backed NumPy\n",
    "x = onp.random.normal(size=(size, size)).astype(onp.float32)\n",
    "%timeit np.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import device_put\n",
    "\n",
    "x = onp.random.normal(size=(size, size)).astype(onp.float32)\n",
    "x = device_put(x)\n",
    "%timeit np.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `jit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * np.where(x > 0, x, alpha * np.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sum_logistic(x):\n",
    "    return np.sum(1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "x = np.arange(3.)\n",
    "print(sum_logistic(x))\n",
    "print(grad(sum_logistic)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.normal(key, (10, 3))\n",
    "batched_sum = vmap(sum_logistic)\n",
    "batched_sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x, y):\n",
    "    return np.sum(x**2 + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y  = [random.normal(key, (10,3)), random.normal(key + 1, (10,3))]\n",
    "print('single argument:', test(x[0], y[0]), '\\n')\n",
    "print('batch output shape:', vmap(test)(x, y).shape)\n",
    "print('batch output:', vmap(test)(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(np.array([1,2,3]), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.split(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = x.split(1)[0]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_v = row.expand_as(x)\n",
    "r_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_dist = torch.sum((r_v - x) ** 2, 1)\n",
    "print(sq_dist.shape)\n",
    "sq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_dist.view(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_pairwise_distances(x, y=None, dist_mat=None):\n",
    "    if y is None:\n",
    "        y = x\n",
    "    if dist_mat is None:\n",
    "        dtype = x.data.type()\n",
    "        dist_mat = Variable(torch.Tensor(x.size()[0], y.size()[0]).type(dtype))\n",
    "\n",
    "    for i, row in enumerate(x.split(1)):\n",
    "        r_v = row.expand_as(y)\n",
    "        sq_dist = torch.sum((r_v - y) ** 2, 1)\n",
    "        dist_mat[i] = sq_dist.view(1, -1)\n",
    "    return dist_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first question\n",
    "does `jit` cache results if it needs them again? that is, does it skip over repeated computations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10**4\n",
    "def h(x):\n",
    "    for i in range(m):\n",
    "        x += i\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    \"\"\"\n",
    "    computation of h is repeated needlessly\n",
    "    \"\"\"\n",
    "    out = 0\n",
    "    for i in range(10):\n",
    "        out += h(x) + i\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    \"\"\"\n",
    "    h(x) is computed only once\n",
    "    \"\"\"\n",
    "    out = 0\n",
    "    hx = h(x)\n",
    "    for i in range(10):\n",
    "        out += hx + i\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert f1(s) == f2(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit f1(s)\n",
    "%timeit f2(s)\n",
    "%timeit jit(f1)(s).block_until_ready()\n",
    "%timeit jit(f2)(s).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next question:\n",
    "does `lax.fori_loop` compile more quickly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10**4\n",
    "s = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h1(x):\n",
    "    body = lambda i, val: val + i\n",
    "    for i in range(m):\n",
    "        x = body(i, x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h2(x):\n",
    "    body = lambda i, val: val + i\n",
    "    return lax.fori_loop(0, m, body, init_val=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit(h1)(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit(h2)(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yeees, it does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next question:\n",
    "when we use `lax.fori_loop`, do we get the same speedup for repeated computations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert h(s) == h1(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we use the lax fori loop h1\n",
    "def f1_lax(x):\n",
    "    \"\"\"\n",
    "    computation of h is repeated needlessly\n",
    "    \"\"\"\n",
    "    out = 0\n",
    "#     for i in range(10):\n",
    "#         out += h1(x) + i\n",
    "        \n",
    "    out = lax.fori_loop(0, 10, lambda i, val: val + h1(x) + i, init_val=out)\n",
    "    return out\n",
    "\n",
    "def f2_lax(x):\n",
    "    \"\"\"\n",
    "    h(x) is computed only once\n",
    "    \"\"\"\n",
    "    out = 0\n",
    "    hx = h1(x)\n",
    "#     for i in range(10):\n",
    "#         out += hx + i\n",
    "        \n",
    "    out = lax.fori_loop(0, 10, lambda i, val: val + hx + i, init_val=out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert f1_lax(s) == f1(s)\n",
    "assert f2_lax(s) == f2(s)\n",
    "assert f1(s) == f2(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still compiles fast:\n",
    "jit(f1_lax)(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit f1_lax(s)\n",
    "%timeit f2_lax(s)\n",
    "%timeit jit(f1_lax)(s).block_until_ready()\n",
    "%timeit jit(f2_lax)(s).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we do indeed have a speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to plot 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data.\n",
    "X = np.linspace(-5, 5, 50)\n",
    "Y = np.linspace(-5, 5, 50)\n",
    "X, Y = np.meshgrid(X, Y) # both shape (40, 40)\n",
    "\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
