{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, random\n",
    "from jax.lax import fori_loop\n",
    "from utils import ard, squared_distance_matrix\n",
    "from jax.ops import index, index_add, index_update\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgd import update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgd import SVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SVGD():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    kernel_param_update_rule = kernel_param_update_rule\n",
    "    \n",
    "    def fixed_param_svgd(self, x, logp, stepsize, L, bandwidth):\n",
    "        \"\"\"\n",
    "        IN:\n",
    "        * x is an np array of shape n x d\n",
    "        * logp is the log of a differentiable pdf p (callable)\n",
    "        * stepsize is a float\n",
    "        * L is an integer (number of iterations)\n",
    "        * bandwidth is a positive scalar: bandwidth parameter for RBF kernel\n",
    "\n",
    "        OUT:\n",
    "        * Updated particles x (np array of shape n x d) after L steps of SVGD\n",
    "        * dictionary with logs\n",
    "        \"\"\"\n",
    "        assert x.ndim == 2\n",
    "\n",
    "        d = x.shape[1]\n",
    "        log = {\n",
    "            \"particle_mean\": np.empty(shape=(L, d)),\n",
    "            \"particle_var\": np.empty(shape=(L, d))\n",
    "        }\n",
    "\n",
    "        def update_fun(i, u):\n",
    "            \"\"\"\n",
    "            1) compute updated x,\n",
    "            2) log mean and var\n",
    "\n",
    "            Parameters:\n",
    "            * i: iteration counter (unused)\n",
    "            * u = [x, log]\n",
    "            \n",
    "            Returns:\n",
    "            [updated_x, log]\n",
    "            \"\"\"\n",
    "            x, log = u\n",
    "            x = update(x, logp, stepsize, bandwidth)\n",
    "\n",
    "            update_dict = {\n",
    "                \"particle_mean\": np.mean(x, axis=0),\n",
    "                \"particle_var\": np.var(x, axis=0)\n",
    "            }\n",
    "\n",
    "            for key in log.keys():\n",
    "                log[key] = index_update(log[key], index[i, :], update_dict[key])\n",
    "                \n",
    "            return [x, log]\n",
    "\n",
    "        x, log = fori_loop(0, L, update_fun, [x, log]) # when I wanna do grad(svgd), I need to reimplement fori_loop using scan (which is differentiable).\n",
    "\n",
    "        return x, log\n",
    "\n",
    "    fixed_param_svgd = jit(fixed_param_svgd, static_argnums=(0, 2, 4))\n",
    "    \n",
    "    \n",
    "    def adaptive_param_svgd(self, x, logp, stepsize, L):\n",
    "        \"\"\"\n",
    "        IN:\n",
    "        * x is an np array of shape n x d\n",
    "        * logp is the log of a differentiable pdf p (callable)\n",
    "        * stepsize is a float\n",
    "        * L is an integer (number of iterations)\n",
    "\n",
    "        OUT:\n",
    "        * Updated particles x (np array of shape n x d) after L steps of SVGD\n",
    "        * dictionary with logs\n",
    "        \"\"\"\n",
    "        assert x.ndim == 2\n",
    "\n",
    "        d = x.shape[1]\n",
    "        log = {\n",
    "            \"kernel_param\": np.empty(shape=(L, d)),\n",
    "            \"particle_mean\": np.empty(shape=(L, d)),\n",
    "            \"particle_var\": np.empty(shape=(L, d))\n",
    "        }\n",
    "\n",
    "        def update_fun(i, u):\n",
    "            \"\"\"\n",
    "            1) compute kernel_param from x\n",
    "            2) compute updated x,\n",
    "            3) log everything\n",
    "\n",
    "            Parameters:\n",
    "            * i: iteration counter (unused)\n",
    "            * u = [x, log]\n",
    "            \"\"\"\n",
    "            x, log = u\n",
    "            kernel_param = kernel_param_update_rule(x)\n",
    "            x = update(x, logp, stepsize, kernel_param)\n",
    "\n",
    "            update_dict = {\n",
    "                \"kernel_param\": kernel_param,\n",
    "                \"particle_mean\": np.mean(x, axis=0),\n",
    "                \"particle_var\": np.var(x, axis=0)\n",
    "            }\n",
    "\n",
    "            for key in log.keys():\n",
    "                log[key] = index_update(log[key], index[i, :], update_dict[key])\n",
    "\n",
    "            return [x, log]\n",
    "\n",
    "        x, log = fori_loop(0, L, update_fun, [x, log])\n",
    "\n",
    "        return x, log\n",
    "\n",
    "    adaptive_param_svgd = jit(adaptive_param_svgd, static_argnums=(0, 2, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "svgd = SVGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logp is located at 0x7f7648115d90\n"
     ]
    }
   ],
   "source": [
    "from jax.scipy.stats import norm\n",
    "\n",
    "@jit\n",
    "def logp(x):\n",
    "    \"\"\"\n",
    "    IN: single scalar np array x. alternatively, [x] works too\n",
    "    OUT: scalar logp(x)\n",
    "    \"\"\"\n",
    "    return np.squeeze(np.sum(norm.logpdf(x, loc=0, scale=1)))\n",
    "\n",
    "print('logp is located at', hex(id(logp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h is located at 0xa68b20\n"
     ]
    }
   ],
   "source": [
    "n = 300\n",
    "stepsize = 0.01\n",
    "\n",
    "L = int(1 / stepsize)\n",
    "\n",
    "# generate data\n",
    "key = random.PRNGKey(7)\n",
    "x0 = random.normal(key, (n,1)) - 10\n",
    "\n",
    "h = 4\n",
    "print('h is located at', hex(id(h))) # memory location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "xout, log = svgd.fixed_param_svgd(x0, logp, stepsize, L, h)\n",
    "el = time.time() - st\n",
    "print(\"elapsed time:\", el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(x0[:, 0])\n",
    "_ = plt.hist(xout[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "xout2, log2 = svgd.adaptive_param_svgd(x0, logp, stepsize, L)\n",
    "el = time.time() - st\n",
    "print(\"elapsed time:\", el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(x0[:, 0])\n",
    "_ = plt.hist(xout2[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elapsed times:\n",
    "* called for first time, nonjitted, n=100: both 80s\n",
    "* called for first time, jitted, n=100: fixed 90s, adaptive 90s (+-10s)\n",
    "* subsequent calls with same `x` data are extremely fast, but recompilation is triggered by reloading `logp`. Interesting, reloading `h` does not trigger recompilation. Upon checking, this is because reassigning `h` doesnt actually do anything to the `h` in memory, but reassigning `logp` to the same function does.\n",
    "* called second time after `jit` compilation (after changing sampled input `x` (and/or bandwidth `h` in fixed param case) but nothing else): just as fast as with no change, on the order of 0.1-1 milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
